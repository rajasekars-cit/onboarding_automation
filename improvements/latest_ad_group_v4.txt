#All files
# ==============================================================================
#run.py
#config.py
#main.py
#ad_service.py
#email_service.py
#db_service.py
#ai_service.py
# ==============================================================================

# FILE: run.py
# ==============================================================================
# Orchestrates multi-system onboarding workflow using thread pool.
# Producer enqueues active configs, workers dequeue and run processing.
# Implements shared mailbox polling + multi-dispatcher pattern.
# ==============================================================================

import time
import logging
import queue
import threading
from concurrent.futures import ThreadPoolExecutor
from app.main import run
from app.services import db_service
from app.config import get_static_config

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(threadName)s - %(module)s - %(message)s'
)

WORK_QUEUE = queue.Queue()

def worker_thread():
    """ Worker thread that dequeues and processes tasks. """
    logging.info("Worker thread started")
    while True:
        config = WORK_QUEUE.get()
        if config is None:
            logging.info("Worker received sentinel to shut down")
            break
        try:
            if 'all_configs' in config and 'static_config' in config:
                # This single task triggers the entire two-phase workflow
                run(config['all_configs'], config['static_config'])
            else:
                logging.warning(f"Unknown or incomplete task received: {config}")
        except Exception as e:
            logging.error(f"Worker task error: {e}", exc_info=True)
        finally:
            WORK_QUEUE.task_done()

def producer_thread(static_config):
    """ Producer thread that schedules workflow runs at a configured interval. """
    while True:
        try:
            logging.info("Producer waking to schedule tasks")
            active_configs = db_service.get_all_active_configurations()
            if not active_configs:
                logging.info("No active configurations found")
            else:
                logging.info(f"Scheduling workflow for {len(active_configs)} active configurations")
                # Bundle all active configs into a single task for the new workflow
                shared_mailbox_task = {
                    'all_configs': active_configs,
                    'static_config': static_config
                }
                WORK_QUEUE.put(shared_mailbox_task)
            
            interval = static_config.get("SCHEDULE_MINUTES", 5) * 60
            time.sleep(interval)
        except Exception as e:
            logging.error(f"Producer thread error: {e}", exc_info=True)
            time.sleep(60) # Wait a minute before retrying on error

if __name__ == "__main__":
    logging.info("Starting onboarding application with thread pool")
    static_cfg = get_static_config()
    max_workers = static_cfg.get("MAX_WORKER_THREADS", 10)

    # Ensure database schema is up to date before starting
    db_service.setup_database()

    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="Worker") as executor:
        for _ in range(max_workers):
            executor.submit(worker_thread)

        producer = threading.Thread(target=producer_thread, args=(static_cfg,), name="Producer")
        producer.daemon = True
        producer.start()

        try:
            # Keep the main thread alive to listen for KeyboardInterrupt
            producer.join()
        except KeyboardInterrupt:
            logging.info("Shutdown requested, signaling workers to stop...")
            for _ in range(max_workers):
                WORK_QUEUE.put(None)

# FILE: app/config.py
# ==============================================================================
# Static configuration loader for environment variables
# ==============================================================================

import os
import logging
from dotenv import load_dotenv

# Load environment variables from a .env file if it exists
load_dotenv()

def get_static_config():
    """Loads static configuration from environment variables."""
    logging.info("Loading static configuration from environment variables")
    config = {}
    config['SCHEDULE_MINUTES'] = int(os.getenv("SCHEDULE_MINUTES", 1))
    config['MAX_WORKER_THREADS'] = int(os.getenv("MAX_WORKER_THREADS", 5))
    config['INITIAL_LOOKBACK_DAYS'] = int(os.getenv("INITIAL_LOOKBACK_DAYS", 1))
    config['REMINDER_THRESHOLD_HOURS'] = int(os.getenv("REMINDER_THRESHOLD_HOURS", 24))
    
    # NEW: Delay for the action worker to process staged requests
    config['MATURITY_DELAY_MINUTES'] = int(os.getenv("MATURITY_DELAY_MINUTES", 2))

    # AI Service Config
    config['OLLAMA_HOST'] = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    config['OLLAMA_MODEL'] = os.getenv("OLLAMA_MODEL", "llama3:8b")
    
    # Azure AD / Graph API Config
    config['AZURE_TENANT_ID'] = os.getenv("AZURE_TENANT_ID")
    config['AZURE_CLIENT_ID'] = os.getenv("AZURE_CLIENT_ID")
    config['AZURE_CLIENT_SECRET'] = os.getenv("AZURE_CLIENT_SECRET")
    
    logging.info("Static configuration loaded.")
    return config

# FILE: app/main.py
# ==============================================================================
# Main onboarding workflow orchestration called by worker threads.
# Implements the new two-phase workflow.
# ==============================================================================

import logging
from app.services.email_service import (
    ingest_emails_to_db, 
    process_pending_actions, 
    process_pending_reminders
)

def run(all_configs, static_config):
    """
    Orchestrates the new two-phase workflow.

    Args:
        all_configs (list): A list of all active dynamic configurations from the DB.
        static_config (dict): A dictionary of static configurations from .env.
    """
    logging.info("Starting two-phase workflow cycle")

    # Phase 1: Ingestion Worker (Read & Record)
    # This phase polls the mailbox and records intents in the database without taking action.
    try:
        logging.info("--- Phase 1: Ingesting Emails ---")
        ingest_emails_to_db(all_configs, static_config)
    except Exception as e:
        logging.error(f"Error during email ingestion phase: {e}", exc_info=True)

    # Phase 2: Action Worker (Wait & Execute)
    # This phase acts on "matured" requests and sends reminders.
    logging.info("--- Phase 2: Processing Pending Actions & Reminders ---")
    for cfg in all_configs:
        try:
            full_cfg = {**static_config, **cfg}
            # Act on mature, unprocessed requests that have been staged for a few minutes
            process_pending_actions(full_cfg)
            # Send reminders for requests that have been pending for a long time
            process_pending_reminders(full_cfg)
        except Exception as e:
            logging.error(f"Error during action/reminder phase for config {cfg.get('config_id')}: {e}", exc_info=True)

    logging.info("Completed two-phase workflow cycle")

# FILE: app/services/ad_service.py
# ==============================================================================
# Service for interacting with Azure Active Directory via Microsoft Graph API.
# Handles token acquisition, user/group lookups, and membership checks.
# ==============================================================================
import logging
import msal
import requests

GRAPH_API_ENDPOINT = 'https://graph.microsoft.com/v1.0'
_app_cache = {} # Cache the MSAL app object to reuse its internal token cache

def get_access_token(config):
    """Acquires an access token for the Microsoft Graph API, using MSAL's built-in caching."""
    tenant_id = config['AZURE_TENANT_ID']
    client_id = config['AZURE_CLIENT_ID']
    
    if not all([tenant_id, client_id, config.get('AZURE_CLIENT_SECRET')]):
        logging.error("Azure AD credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) are not configured.")
        return None

    # Reuse the app object to leverage MSAL's internal token cache
    if client_id not in _app_cache:
        authority = f"https://login.microsoftonline.com/{tenant_id}"
        app = msal.ConfidentialClientApplication(
            client_id=client_id,
            authority=authority,
            client_credential=config['AZURE_CLIENT_SECRET'],
        )
        _app_cache[client_id] = app
    
    app = _app_cache[client_id]
    
    # acquire_token_for_client will automatically use its cache and refresh if needed.
    token_result = app.acquire_token_for_client(scopes=["https://graph.microsoft.com/.default"])

    if "access_token" in token_result:
        return token_result["access_token"]
    else:
        logging.error(f"Failed to acquire Graph API token: {token_result.get('error_description')}")
        # Clear the app from cache in case of persistent auth failure
        if client_id in _app_cache:
            del _app_cache[client_id]
        return None

def get_user_id(user_email, token):
    """Gets the Azure AD object ID for a user from their email."""
    headers = {"Authorization": f"Bearer {token}"}
    params = {"$filter": f"mail eq '{user_email}' or userPrincipalName eq '{user_email}'"}
    response = requests.get(f"{GRAPH_API_ENDPOINT}/users", headers=headers, params=params)
    if response.status_code == 200:
        data = response.json().get("value")
        if data: return data[0]["id"]
    return None

def get_group_id(group_name, token):
    """Gets the Azure AD object ID for a group from its display name."""
    headers = {"Authorization": f"Bearer {token}"}
    params = {"$filter": f"displayName eq '{group_name}'"}
    response = requests.get(f"{GRAPH_API_ENDPOINT}/groups", headers=headers, params=params)
    if response.status_code == 200:
        data = response.json().get("value")
        if data: return data[0]["id"]
    return None

def is_user_in_group(user_email, group_name, config):
    """Checks if a user is a member of a specific Azure AD group."""
    token = get_access_token(config)
    if not token: return False
    group_id = get_group_id(group_name, token)
    if not group_id:
        logging.error(f"AD group '{group_name}' not found.")
        return False
    user_id = get_user_id(user_email, token)
    if not user_id:
        logging.warning(f"User '{user_email}' not found in AD for group check.")
        return False
    headers = {"Authorization": f"Bearer {token}"}
    json_payload = {"groupIds": [group_id]}
    response = requests.post(f"{GRAPH_API_ENDPOINT}/users/{user_id}/checkMemberGroups", headers=headers, json=json_payload)
    if response.status_code == 200:
        if group_id in response.json().get("value", []):
            logging.info(f"AD check PASSED: {user_email} is a member of '{group_name}'.")
            return True
    logging.warning(f"AD check FAILED: {user_email} is NOT a member of '{group_name}'.")
    return False

def get_user_manager(user_email, config):
    """Fetches the line manager's email for a given user from Azure AD."""
    token = get_access_token(config)
    if not token: return None
    user_id = get_user_id(user_email, token)
    if not user_id:
        logging.warning(f"Could not find user ID for '{user_email}'. Cannot fetch manager.")
        return None

    headers = {'Authorization': f'Bearer {token}'}
    response = requests.get(f"{GRAPH_API_ENDPOINT}/users/{user_id}/manager", headers=headers)

    if response.status_code == 200:
        manager_data = response.json()
        manager_email = manager_data.get('mail')
        if manager_email:
            logging.info(f"Found manager for {user_email}: {manager_email}")
            return manager_email.lower()
    logging.warning(f"Could not fetch manager for {user_email}. Status: {response.status_code}, Body: {response.text}")
    return None

def get_group_owners(group_name, config):
    """Fetches the email addresses of the owners of a specific AD group."""
    token = get_access_token(config)
    if not token: return []
    group_id = get_group_id(group_name, token)
    if not group_id:
        logging.error(f"Cannot get owners because AD group '{group_name}' was not found.")
        return []
    headers = {'Authorization': f'Bearer {token}'}
    response = requests.get(f"{GRAPH_API_ENDPOINT}/groups/{group_id}/owners", headers=headers)
    if response.status_code == 200:
        owners_data = response.json().get("value", [])
        owner_emails = [owner.get('mail').lower() for owner in owners_data if owner.get('mail')]
        logging.info(f"Found owners for group '{group_name}': {owner_emails}")
        return owner_emails
    logging.error(f"Error fetching owners for group '{group_name}': {response.text}")
    return []

# FILE: app/services/email_service.py
# ==============================================================================
# Implements the two-phase workflow:
# 1. ingest_emails_to_db: Reads emails and records intent in the DB (no actions).
# 2. process_pending_actions: Acts on "matured" requests from the DB.
# ==============================================================================

import imaplib
import email
from email.header import decode_header
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import logging
import re
from datetime import datetime

from app.services import db_service, ad_service
from app.services.ai_service import analyze_email

# Regular expression for matching automated email addresses to ignore them.
AUTO_ADDRESS_PATTERNS = [
    r'no-?reply@', r'notification', r'do-?not-?reply@', r'mailer-daemon', r'postmaster@',
    r'automated', r'helpdesk', r'bounces@', r'^noreply', r'bot@', r'listserv',
    r'system@', r'alerts?@'
]

# --- Utility Functions ---

def is_autogenerated_address(email_address):
    """Checks if an email address matches common patterns for automated senders."""
    if not email_address:
        return True
    return any(re.search(pat, email_address, re.IGNORECASE) for pat in AUTO_ADDRESS_PATTERNS)

def extract_email(from_header):
    """Extracts a clean email address from a 'From' header string."""
    if not from_header:
        return None
    match = re.search(r'<([^>]+)>', from_header)
    if match:
        return match.group(1).strip().lower()
    match = re.search(r'[\w.\-+]+@[\w.\-]+', from_header)
    if match:
        return match.group(0).strip().lower()
    return None

def get_email_body(msg):
    """Extracts the plain text body from an email message object."""
    if msg.is_multipart():
        for part in msg.walk():
            content_type = part.get_content_type()
            content_disp = str(part.get("Content-Disposition") or "")
            if content_type == "text/plain" and "attachment" not in content_disp.lower():
                try:
                    return part.get_payload(decode=True).decode()
                except (UnicodeDecodeError, AttributeError):
                    return part.get_payload(decode=True).decode('latin-1', errors='ignore')
    else:
        try:
            return msg.get_payload(decode=True).decode()
        except (UnicodeDecodeError, AttributeError):
            return msg.get_payload(decode=True).decode('latin-1', errors='ignore')
    return ""

def build_search_query(last_check_timestamp):
    """Builds the IMAP search query based on the last check timestamp."""
    search_date = datetime.fromisoformat(last_check_timestamp).strftime('%d-%b-%Y')
    return f'(SINCE "{search_date}")'

def send_email(recipients, subject, body, config):
    """Sends an email using SMTP configuration."""
    msg = MIMEMultipart()
    msg['From'] = f"{config['team_alias']} <{config['smtp_user']}>"
    msg['To'] = ", ".join(recipients)
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'plain'))
    try:
        with smtplib.SMTP(config['smtp_server'], config['smtp_port']) as server:
            server.starttls()
            server.login(config['smtp_user'], config['smtp_pass'])
            server.send_message(msg)
            logging.info(f"Sent email '{subject}' to {recipients}")
    except Exception as e:
        logging.error(f"Failed to send email '{subject}' to {recipients}: {e}", exc_info=True)

def build_group_config_map(config_list, static_config):
    """Creates a dictionary mapping a requested AD group to its full configuration."""
    return {
        (c.get('required_ad_group') or '').upper(): {**static_config, **c}
        for c in config_list if c.get('required_ad_group')
    }

# ==============================================================================
# Phase 1: Ingestion Worker (Read & Record)
# ==============================================================================

def ingest_emails_to_db(config_list, static_config):
    """
    PHASE 1: Polls the shared mailbox, analyzes emails, and records intents in the database.
    """
    if not config_list:
        logging.warning("No configs provided for ingestion.")
        return

    shared_config = {**static_config, **config_list[0]}
    config_id = 'SHARED_MAILBOX'
    last_check_timestamp = db_service.get_last_check_time(config_id, shared_config)
    group_config_map = build_group_config_map(config_list, static_config)

    try:
        with imaplib.IMAP4_SSL(shared_config['imap_server']) as mail:
            mail.login(shared_config['imap_user'], shared_config['imap_pass'])
            mail.select("INBOX")
            search_query = build_search_query(last_check_timestamp)
            status, message_data = mail.uid('search', None, search_query)
            if status != "OK":
                logging.error(f"[{config_id}] IMAP search failed.")
                return

            all_uids = message_data[0].split()
            if all_uids:
                logging.info(f"Found {len(all_uids)} new emails to process.")
            for uid in all_uids:
                uid_str = uid.decode()
                if not db_service.claim_uid_for_processing(uid_str):
                    continue

                try:
                    fetch_status, msg_parts = mail.uid('fetch', uid, '(RFC822)')
                    if fetch_status != 'OK' or not msg_parts or not msg_parts[0]:
                        logging.error(f"Failed to fetch email content for UID {uid_str}. Status: {fetch_status}. The email may be corrupted or unavailable on the server.")
                        continue

                    msg = email.message_from_bytes(msg_parts[0][1])
                    subject, _ = decode_header(msg.get("Subject") or "")[0]
                    if isinstance(subject, bytes): subject = subject.decode('utf-8', 'ignore')
                    
                    sender_email = extract_email(msg.get("From"))
                    if not sender_email or is_autogenerated_address(sender_email):
                        logging.info(f"Skipping email from invalid/automated sender: {sender_email}")
                        continue

                    body = get_email_body(msg)
                    analysis = analyze_email(subject, body, shared_config)
                    
                    if not analysis:
                        logging.warning(f"AI analysis for UID {uid_str} failed or returned no result. Skipping. Check AI service connectivity and logs.")
                        continue

                    intent = analysis.get('intent')
                    user_email = analysis.get('user_email')

                    if intent == 'new_request':
                        requested_group = (analysis.get('requested_group') or '').upper()
                        if not user_email or not requested_group:
                            logging.warning(f"Missing user_email or requested_group in new request. Skipping UID {uid_str}.")
                            continue
                        
                        target_config = group_config_map.get(requested_group)
                        if not target_config:
                            logging.warning(f"No config found for group '{requested_group}'. Skipping UID {uid_str}.")
                            continue

                        logging.info(f"Staging new request for user '{user_email}', group '{requested_group}'.")
                        db_service.create_onboarding_request_composite(
                            user_email=user_email,
                            group=requested_group,
                            config_id=target_config['config_id']
                        )

                    elif intent == 'approval':
                        if not user_email:
                            logging.warning(f"Missing user_email in approval email. Skipping UID {uid_str}.")
                            continue

                        active_request = db_service.find_active_request_by_user(user_email)
                        if active_request:
                            logging.info(f"Processing approval from '{sender_email}' for user '{user_email}'.")
                            target_config = group_config_map.get(active_request['requested_group'].upper())
                            if target_config:
                                handle_approval_email(
                                    from_email=sender_email,
                                    user_email=user_email,
                                    requested_group=active_request['requested_group'],
                                    config=target_config
                                )
                        else:
                            logging.warning(f"Approval received from '{sender_email}' for '{user_email}', but no active request was found.")
                    else:
                        logging.info(f"Intent '{intent}' requires no action in ingestion phase. Skipping UID {uid_str}.")

                except Exception as e:
                    logging.error(f"[{config_id}] Exception processing UID {uid_str}: {e}", exc_info=True)

            db_service.update_last_check_time(config_id, datetime.now().isoformat())
    except Exception as e:
        logging.error(f"[{config_id}] Could not access or process mailbox: {e}", exc_info=True)


# ==============================================================================
# Phase 2: Action Worker (Wait & Execute)
# ==============================================================================

def process_pending_actions(config):
    """
    PHASE 2: Queries for "matured" requests (status='new_unprocessed') and acts on them.
    """
    config_id = config['config_id']
    try:
        mature_requests = db_service.get_mature_unprocessed_requests(config_id, config)
        if mature_requests:
             logging.info(f"[{config_id}] Found {len(mature_requests)} mature requests to process.")
        
        for request in mature_requests:
            user_email = request['user_to_onboard_email']
            requested_group = request['requested_group']
            logging.info(f"[{config_id}] Evaluating mature request for user '{user_email}'.")

            missing_approvers = db_service.get_missing_approvers_for_stage(request, config)

            if not missing_approvers:
                logging.info(f"[{config_id}] All approvals present for '{user_email}'. Proceeding to onboard.")
                db_service.onboard_user_to_target_db(user_email, config)
                db_service.update_request_status_composite(
                    user_email, requested_group, config_id, 'completed', 
                    'Access granted via pre-approved request.'
                )
                send_confirmation_email(user_email, config)
            else:
                logging.info(f"[{config_id}] Approvals missing for '{user_email}'. Initiating approval process.")
                send_request_to_next_stage(request, config)

    except Exception as e:
        logging.error(f"[{config_id}] Error processing pending actions: {e}", exc_info=True)


# --- Approval and Reminder Logic ---

def handle_approval_email(from_email, user_email, requested_group, config):
    """
    Handles an incoming approval email, records it, and advances the workflow.
    """
    config_id = config.get('config_id', 'DEFAULT')
    request = db_service.get_active_request(user_email, requested_group, config_id)
    if not request:
        logging.warning(f"[{config_id}] Approval received but no active request found for {user_email}.")
        return

    effective_approvers = db_service.get_effective_approvers_for_stage(request, config)
    
    if not effective_approvers:
        logging.error(f"[{config_id}] Could not determine any effective approvers for request ID {request['id']} at stage {request['current_stage']}. "
                      f"Check AD configuration for user '{user_email}' and group '{config.get('required_ad_group')}'.")
        return

    if from_email not in effective_approvers:
        logging.warning(f"[{config_id}] Unauthorized approval attempt by '{from_email}' for request ID {request['id']}. "
                        f"Effective approvers are: {effective_approvers}.")
        return

    db_service.add_stage_approval(request, from_email, config)

    if request['status'] == 'new_unprocessed':
        logging.info(f"[{config_id}] Recorded proactive approval for staged request for {user_email}. Action worker will process.")
        return
        
    request = db_service.get_active_request(user_email, requested_group, config_id) # Reload request
    
    missing = db_service.get_missing_approvers_for_stage(request, config)
    if not missing:
        logging.info(f"[{config_id}] Stage {request['current_stage']} for {user_email} is now fully approved.")
        
        # Check if this is the final stage
        if request['current_stage'] >= 2: # Assuming 2 is the final approval stage
            db_service.onboard_user_to_target_db(user_email, config)
            db_service.update_request_status_composite(user_email, requested_group, config_id, 'completed', 'Access granted after final approval.')
            send_confirmation_email(user_email, config)
        else:
            # ===== FIX STARTS HERE =====
            # Instead of blindly sending a new request, advance the stage and then
            # re-evaluate the request in its new state.
            logging.info(f"[{config_id}] Advancing request {request['id']} to the next stage.")
            advanced_request = db_service.advance_to_next_stage_composite(user_email, requested_group, config_id)
            if advanced_request:
                logging.info(f"[{config_id}] Re-evaluating approvals for newly advanced stage {advanced_request['current_stage']}.")
                # Recursive call to check if the new stage is also already approved
                handle_approval_email(from_email, user_email, requested_group, config)
            # ===== FIX ENDS HERE =====
    else:
        logging.info(f"[{config_id}] Approval from {from_email} recorded. Still waiting on: {missing}")

def process_pending_reminders(config):
    """Sends reminders for requests that have been pending for too long."""
    try:
        requests = db_service.get_pending_requests_for_reminder(config)
        for req in requests:
            missing_approvers = db_service.get_missing_approvers_for_stage(req, config)
            if missing_approvers:
                subject = f"REMINDER: Approval Required for {req['user_to_onboard_email']}"
                body = (
                    f"Hello,\n\nThis is a reminder that your approval is still required for user "
                    f"'{req['user_to_onboard_email']}' to access '{req['requested_group']}'.\n\n"
                    f"Thank you,\n{config['team_alias']}"
                )
                send_email(missing_approvers, subject, body, config)
                db_service.update_request_status_composite(
                    req['user_to_onboard_email'], req['requested_group'], req['config_id'],
                    req['status'], f"Reminder sent to: {', '.join(missing_approvers)}"
                )
    except Exception as e:
        logging.error(f"[{config['config_id']}] Get pending reminders failed: {e}")

# --- Email Sending Helpers ---

def send_confirmation_email(user_email, config):
    subject = f"Welcome! Your Access to {config['team_alias']} is Granted"
    body = f"Hello,\n\nYour account ({user_email}) has been successfully onboarded.\n\nBest regards,\n{config['team_alias']}"
    send_email([user_email], subject, body, config)

def send_request_to_next_stage(request, config):
    """Sends an approval request email to the required approvers for the current stage."""
    stage_num = request['current_stage']
    user_email = request['user_to_onboard_email']
    group = request['requested_group']
    
    approvers = db_service.get_required_approvers_for_stage(request, config)
    stage_name = "Manager" if stage_num == 1 else f"Stage {stage_num} Owner"
    
    if not approvers:
        logging.error(f"Could not find approvers for {stage_name} for user {user_email}.")
        db_service.update_request_status_composite(
            user_email, group, config['config_id'], 'error', f"No approvers found for stage {stage_num}"
        )
        return

    subject = f"ACTION REQUIRED: Approve Onboarding for {user_email}"
    body = (
        f"Hello,\n\nThe user '{user_email}' has requested access to the '{group}' system, "
        f"which requires your approval.\n\nPlease reply to this email with 'Approved' or 'Rejected'.\n\n"
        f"Thank you,\n{config['team_alias']}"
    )
    send_email(approvers, subject, body, config)
    
    new_status = f"pending_{stage_name.lower().replace(' ','_')}_approval"
    db_service.update_request_status_composite(
        user_email, group, config['config_id'], new_status, 
        f"Approval request sent to {stage_name}(s): {', '.join(approvers)}"
    )

# FILE: app/services/db_service.py
# ==============================================================================
# Database interaction layer for user onboarding requests.
# Implements logic for the new two-phase workflow, including staging requests
# and handling a 30-second timestamp overlap for email fetching.
# ==============================================================================

import psycopg2
import logging
import json
import os
from psycopg2.extras import DictCursor
from datetime import datetime, timedelta
import mysql.connector
import oracledb
import pyodbc
from app.services import ad_service

def get_db_connection():
    """Establishes and returns a connection to the main PostgreSQL database."""
    try:
        return psycopg2.connect(
            dbname=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASS"),
            host=os.getenv("DB_HOST"),
            port=os.getenv("DB_PORT")
        )
    except Exception as e:
        logging.error(f"Failed to connect to application DB: {e}")
        raise

def setup_database():
    """Initializes all necessary tables and triggers in the database."""
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS onboarding_tracker (
                    id SERIAL PRIMARY KEY,
                    user_to_onboard_email VARCHAR(255) NOT NULL,
                    requested_group VARCHAR(100) NOT NULL,
                    config_id TEXT NOT NULL,
                    status VARCHAR(50) NOT NULL DEFAULT 'new_unprocessed',
                    current_stage INT DEFAULT 1,
                    stage_approvals JSONB DEFAULT '{}'::jsonb,
                    delegated_approvers JSONB DEFAULT '[]'::jsonb,
                    duplicate_of INTEGER,
                    request_count INT DEFAULT 1,
                    last_activity_details TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_onboard_email_group_config ON onboarding_tracker(user_to_onboard_email, requested_group, config_id);")
            cur.execute("CREATE TABLE IF NOT EXISTS onboarding_log (email TEXT NOT NULL, config_id TEXT NOT NULL, access_flag BOOLEAN DEFAULT FALSE, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (email, config_id));")
            cur.execute("CREATE TABLE IF NOT EXISTS processed_uids (uid TEXT PRIMARY KEY, processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP);")
            cur.execute("CREATE TABLE IF NOT EXISTS app_state (key TEXT PRIMARY KEY, value TEXT NOT NULL);")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS configuration (
                    config_id TEXT PRIMARY KEY, description TEXT, is_active BOOLEAN DEFAULT TRUE,
                    team_alias TEXT NOT NULL, imap_server TEXT NOT NULL, imap_user TEXT NOT NULL,
                    imap_pass TEXT NOT NULL, smtp_server TEXT NOT NULL, smtp_port INT NOT NULL,
                    smtp_user TEXT NOT NULL, smtp_pass TEXT NOT NULL, workflow_type TEXT NOT NULL,
                    required_ad_group TEXT, target_db_type TEXT, target_db_config JSONB,
                    target_table_name TEXT, target_column_mappings JSONB
                );
            """)
            cur.execute("""
                CREATE OR REPLACE FUNCTION trigger_set_timestamp() RETURNS TRIGGER AS $$
                BEGIN NEW.updated_at = NOW(); RETURN NEW; END;
                $$ LANGUAGE plpgsql;
            """)
            cur.execute("DROP TRIGGER IF EXISTS set_timestamp ON onboarding_tracker;")
            cur.execute("CREATE TRIGGER set_timestamp BEFORE UPDATE ON onboarding_tracker FOR EACH ROW EXECUTE PROCEDURE trigger_set_timestamp();")
            conn.commit()
            logging.info("Database setup checked and completed.")

def get_last_check_time(config_id, config):
    """
    Gets the last check timestamp for a given config, applying a 30-second safety overlap.
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            key = f"last_check_timestamp_{config_id}"
            cur.execute("SELECT value FROM app_state WHERE key = %s;", (key,))
            row = cur.fetchone()
            if row and row[0]:
                last_time = datetime.fromisoformat(row[0])
                overlapped_time = last_time - timedelta(seconds=30)
                logging.info(f"[{config_id}] Last check: {last_time}. Using overlap time: {overlapped_time}")
                return overlapped_time.isoformat()

            fallback = (datetime.now() - timedelta(days=config.get('INITIAL_LOOKBACK_DAYS', 1))).isoformat()
            logging.info(f"[{config_id}] No last check timestamp found, using fallback: {fallback}")
            return fallback

def update_last_check_time(config_id, timestamp_iso):
    """Updates the last check timestamp for a given config."""
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            key = f"last_check_timestamp_{config_id}"
            cur.execute("INSERT INTO app_state (key, value) VALUES (%s, %s) ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value;", (key, timestamp_iso))
            conn.commit()

def create_onboarding_request_composite(user_email, group, config_id, status='new_unprocessed', current_stage=1, stage_approvals=None):
    """
    Creates a new request record. Defaults to 'new_unprocessed' status for the new workflow.
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                "INSERT INTO onboarding_tracker (user_to_onboard_email, requested_group, config_id, status, current_stage, stage_approvals) VALUES (%s, %s, %s, %s, %s, %s) RETURNING id;",
                (user_email, group, config_id, status, current_stage, json.dumps(stage_approvals or {}))
            )
            request_id = cur.fetchone()[0]
            conn.commit()
            logging.info(f"Created request ID {request_id} for {user_email} with status '{status}'.")
            return request_id

def find_active_request_by_user(user_email):
    """Finds the most recent, non-completed/duplicate request for a user across all configs."""
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute(
                "SELECT * FROM onboarding_tracker WHERE user_to_onboard_email = %s AND status NOT IN ('completed', 'duplicate', 'error') ORDER BY updated_at DESC LIMIT 1;",
                (user_email,)
            )
            return cur.fetchone()

def get_mature_unprocessed_requests(config_id, config):
    """
    Fetches requests that are in 'new_unprocessed' state and older than the configured maturity delay.
    """
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            delay_minutes = config.get('MATURITY_DELAY_MINUTES', 5)
            cur.execute(
                "SELECT * FROM onboarding_tracker WHERE config_id = %s AND status = 'new_unprocessed' AND created_at < NOW() - INTERVAL '%s minutes';",
                (config_id, delay_minutes)
            )
            return cur.fetchall()

def onboard_user_to_target_db(user_email, config):
    """
    Handles the final step of provisioning the user in the target database.
    """
    update_internal_user_access(user_email, config['config_id'])
    if not all([config.get('target_db_type'), config.get('target_table_name'), config.get('target_column_mappings')]):
        logging.warning(f"[{config['config_id']}] No target DB configured, skipping final onboarding.")
        return
    conn = None
    try:
        db_type = config['target_db_type']
        db_config = config['target_db_config']
        if db_type == 'postgresql':
            conn = psycopg2.connect(**db_config)
        elif db_type == 'mysql':
            conn = mysql.connector.connect(**db_config)
        elif db_type == 'oracle':
            conn = oracledb.connect(**db_config)
        elif db_type == 'mssql':
            conn_str = ';'.join([f'{k}={v}' for k, v in db_config.items()])
            conn = pyodbc.connect(conn_str)
        else:
            logging.error(f"Unsupported target DB type: {db_type}")
            return

        cursor = conn.cursor()
        table = config['target_table_name']
        mappings = config['target_column_mappings']
        email_col = mappings['email_column']

        cursor.execute(f"SELECT {email_col} FROM {table} WHERE {email_col} = %s", (user_email,))
        exists = cursor.fetchone()

        default_values = {mappings[k.replace('default_', '') + '_column']: v for k, v in mappings.items() if k.startswith('default_')}

        if exists:
            update_cols = {**default_values, **{mappings['active_column']: True}}
            set_clause = ', '.join([f"{col} = %s" for col in update_cols.keys()])
            query = f"UPDATE {table} SET {set_clause} WHERE {email_col} = %s"
            params = list(update_cols.values()) + [user_email]
        else:
            insert_cols = {**default_values, **{mappings['email_column']: user_email, mappings['active_column']: True}}
            cols_clause = ', '.join(insert_cols.keys())
            placeholders = ', '.join(['%s'] * len(insert_cols))
            query = f"INSERT INTO {table} ({cols_clause}) VALUES ({placeholders})"
            params = list(insert_cols.values())

        cursor.execute(query, params)
        conn.commit()
        logging.info(f"User {user_email} onboarded in target table {table}")
    except Exception as e:
        logging.error(f"Onboarding to target DB failed: {e}", exc_info=True)
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()

def get_required_approvers_for_stage(request, config):
    """Gets the base list of required approvers from AD (manager or group owners)."""
    stage = request['current_stage']
    if stage == 1:
        mgr = ad_service.get_user_manager(request['user_to_onboard_email'], config)
        return [mgr.lower()] if mgr else []
    elif stage == 2:
        return ad_service.get_group_owners(config['required_ad_group'], config)
    return []

# ===== FIX STARTS HERE =====
# The following function was missing and has been restored.
def get_effective_approvers_for_stage(request, config):
    """
    Calculates the list of people who can actually approve, including delegations.
    """
    required = set(get_required_approvers_for_stage(request, config))
    delegations_raw = request.get('delegated_approvers', '[]')
    delegations = json.loads(delegations_raw) if isinstance(delegations_raw, str) else delegations_raw
    
    if not delegations:
        return list(required)
        
    mapping = {item['original'].lower(): item['delegate'].lower() for item in delegations}
    effective = set()
    for approver in required:
        effective.add(mapping.get(approver, approver))
    return list(effective)
# ===== FIX ENDS HERE =====

def get_missing_approvers_for_stage(request, config):
    """Compares effective approvers with those who have already approved."""
    effective = set(get_effective_approvers_for_stage(request, config))
    stage_approvals = request.get('stage_approvals', {})
    approved_for_stage = set(stage_approvals.get(str(request['current_stage']), []))
    return list(effective - approved_for_stage)

def add_stage_approval(request, approver_email, config):
    """Adds an approver's email to the JSONB field for the current stage of a request."""
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            stage_str = str(request['current_stage'])
            stage_approvals = request.get('stage_approvals', {})
            current_approvals = stage_approvals.get(stage_str, [])
            
            if approver_email in current_approvals:
                return False

            stage_approvals[stage_str] = current_approvals + [approver_email.lower()]
            cur.execute(
                "UPDATE onboarding_tracker SET stage_approvals = %s::jsonb, last_activity_details = %s WHERE id = %s;",
                (json.dumps(stage_approvals), f"Approval recorded from {approver_email}", request['id'])
            )
            conn.commit()
            logging.info(f"Added approval from {approver_email} for request ID {request['id']} at stage {stage_str}.")
            return cur.rowcount > 0

def update_request_status_composite(user_email, group, config_id, status, details):
    """Updates the status and details of the active request for a user/group/config."""
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                "UPDATE onboarding_tracker SET status=%s, last_activity_details=%s WHERE user_to_onboard_email=%s AND requested_group=%s AND config_id=%s AND status != 'duplicate';",
                (status, details, user_email, group, config_id)
            )
            conn.commit()

def get_active_request(user_email, group, config_id):
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("SELECT * FROM onboarding_tracker WHERE user_to_onboard_email=%s AND requested_group=%s AND config_id=%s AND status NOT IN ('completed', 'duplicate', 'error') ORDER BY created_at DESC LIMIT 1;", (user_email, group, config_id))
            return cur.fetchone()

def advance_to_next_stage_composite(user_email, group, config_id):
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("UPDATE onboarding_tracker SET current_stage = current_stage + 1 WHERE user_to_onboard_email = %s AND requested_group = %s AND config_id = %s AND status NOT IN ('duplicate', 'completed') RETURNING *;", (user_email, group, config_id))
            req = cur.fetchone()
            conn.commit()
            return req

def get_pending_requests_for_reminder(config):
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            reminder_hours = config.get('REMINDER_THRESHOLD_HOURS', 24)
            cur.execute("SELECT * FROM onboarding_tracker WHERE status LIKE 'pending_%%' AND updated_at < NOW() - INTERVAL '%s hours' AND config_id = %s;", (reminder_hours, config['config_id']))
            return cur.fetchall()

def claim_uid_for_processing(uid):
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute("INSERT INTO processed_uids (uid) VALUES (%s) ON CONFLICT (uid) DO NOTHING;", (uid,))
            conn.commit()
            return cur.rowcount > 0

def get_all_active_configurations():
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("SELECT * FROM configuration WHERE is_active = TRUE;")
            return cur.fetchall()

def update_internal_user_access(user_email, config_id):
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute("INSERT INTO onboarding_log (email, config_id, access_flag) VALUES (%s, %s, TRUE) ON CONFLICT (email, config_id) DO UPDATE SET access_flag=TRUE;", (user_email, config_id))
            conn.commit()

# FILE: app/services/ai_service.py
# ==============================================================================
# AI service for analyzing email content using Ollama.
# Determines intent (new_request, approval, etc.) and extracts key information.
# ==============================================================================
import logging
import json
import re
import ollama

client = None

def is_real_user_email(email_address):
    """Helper to filter out no-reply/bot/system email addresses."""
    if not email_address: return False
    patterns = [
        r'no-?reply@', r'notification', r'do-?not-?reply@', r'mailer-daemon', r'postmaster@',
        r'automated', r'helpdesk', r'bounces@', r'^noreply', r'bot@', r'listserv',
        r'system@', r'alerts?@'
    ]
    return not any(re.search(pat, email_address, re.IGNORECASE) for pat in patterns)

KEYWORDS = ['onboard', 'request access', 'join', 'add access', 'add to group', 'registration', 'enable access', 'new user', 'account setup', 'provision', 'grant access', 'request membership', 'add user']

def contains_onboarding_keyword(text):
    """Checks if text contains common onboarding-related keywords."""
    return any(kw in text.lower() for kw in KEYWORDS)

def analyze_email(subject, body, config):
    """
    Analyzes email content using an LLM to determine intent and extract entities.
    """
    global client
    if client is None:
        logging.info(f"Initializing Ollama client with host: {config['OLLAMA_HOST']}")
        client = ollama.Client(host=config['OLLAMA_HOST'])

    full_content = f"Subject: {subject}\n\nBody:\n{body}"
    compacted = re.sub(r'\s+', ' ', full_content).strip()[:5000]
    
    # RESTORED: Using the original, more detailed prompt for higher accuracy.
    system_prompt = """
You are a careful IT onboarding gatekeeper. Your job is to classify incoming emails.
You MUST ONLY return a JSON dictionary, and nothing else.

A "new_request" is valid ONLY when BOTH:
* The email expresses a clear onboarding intent (contains keywords like "onboard", "request access", "add user", "join", "add to group", "enable access", "registration"), AND
* Contains a real person's email address (NOT no-reply, notification, bot, mailer-daemon, etc).

If BOTH these conditions are not met, classify as intent "query" and set all extracted fields to null.

JSON format for output:

For onboarding requests:
  {
      "intent": "new_request",
      "user_email": "[REAL_EMAIL]",
      "requested_group": "[GROUP]" // The Team/System requested, e.g. "DEV". If you can't find it, set it to null.
  }

For everything else:
  {
    "intent": "query",
    "user_email": null,
    "delegate_email": null,
    "requested_group": null
  }
Do NOT guess or invent values. ONLY extract real emails from the body or subject and carefully check the sender.
If the only email present is a no-reply, notification, daemon, or other bot/system address, DO NOT create new_request.
Never use example.com or placeholder values.

For approvals and other flows: (same as before)
      {"intent": "[approval_or_rejection]", "user_email": "[USER]", "requested_group": "[GROUP_NAME]"}
      (if not found, use nulls)

If you see an “out of office” response that names a delegate, return:
      {"intent": "out_of_office", "delegate_email": "[DELEGATE_EMAIL]"}

REMEMBER:
* If the message does not contain onboarding keywords AND a real user email, classify as "query".
"""

    try:
        response = client.chat(
            model=config['OLLAMA_MODEL'],
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': compacted}
            ],
            options={'temperature': 0.0},
            format='json'
        )
        result_json_str = response['message']['content']
        res = json.loads(result_json_str)

        # Post-processing validation to ensure high-quality results
        user_email = res.get('user_email')
        if res.get('intent') == 'new_request':
            if not (is_real_user_email(user_email) and contains_onboarding_keyword(subject + ' ' + body)):
                logging.warning(f"AI classified as 'new_request' but failed validation. Reverting to 'query'.")
                res['intent'] = "query"
                res['user_email'] = None
                res['requested_group'] = None
                
        logging.info(f"AI Analysis Result: {json.dumps(res)}")
        return res

    except Exception as e:
        logging.error(f"Error calling Ollama or parsing its response: {e}", exc_info=True)
        return None
