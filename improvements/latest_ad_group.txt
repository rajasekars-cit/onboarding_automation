# FILE: run.py
# ==============================================================================
# This file orchestrates the multi-system workflow using a thread pool.
# ==============================================================================
import time
import logging
import queue
import threading
from concurrent.futures import ThreadPoolExecutor
from app.main import run_onboarding_process
from app.services.db_service import setup_database, get_all_active_configurations
from app.config import get_static_config

# --- Basic Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(threadName)s - %(module)s - %(message)s')

# --- Thread Pool Configuration ---
WORK_QUEUE = queue.Queue()

def worker_thread():
    """The consumer function that each thread in the pool will run."""
    logging.info("Worker thread started and waiting for tasks.")
    while True:
        try:
            config = WORK_QUEUE.get()
            if config is None:  # Sentinel value to stop the thread
                break
            run_onboarding_process(config)
        except Exception as e:
            logging.error(f"Error in worker thread: {e}", exc_info=True)
        finally:
            WORK_QUEUE.task_done()

def producer_thread(static_config):
    """The producer function that runs on a schedule to check the DB and add tasks to the queue."""
    while True:
        try:
            logging.info("Producer thread waking up to schedule tasks...")
            active_configs = get_all_active_configurations()
            if not active_configs:
                logging.info("Producer found no active configurations.")
            else:
                logging.info(f"Producer found {len(active_configs)} active configurations to queue.")
                for dynamic_config in active_configs:
                    full_config = {**static_config, **dynamic_config}
                    WORK_QUEUE.put(full_config)
            
            schedule_interval_seconds = int(static_config.get('SCHEDULE_MINUTES', 5)) * 60
            time.sleep(schedule_interval_seconds)
        except Exception as e:
            logging.error(f"Error in producer thread: {e}", exc_info=True)
            time.sleep(60) # Wait a minute before retrying on error

if __name__ == "__main__":
    logging.info("Initializing multi-system onboarding application with Thread Pool...")
    
    static_config = get_static_config()
    MAX_WORKER_THREADS = static_config['MAX_WORKER_THREADS']
    
    setup_database()

    with ThreadPoolExecutor(max_workers=MAX_WORKER_THREADS, thread_name_prefix='Worker') as executor:
        for _ in range(MAX_WORKER_THREADS):
            executor.submit(worker_thread)

        producer = threading.Thread(target=producer_thread, args=(static_config,), name="Producer")
        producer.daemon = True
        producer.start()
        
        try:
            producer.join()
        except KeyboardInterrupt:
            logging.info("Application shutting down...")
            for _ in range(MAX_WORKER_THREADS):
                WORK_QUEUE.put(None)

# ==============================================================================
# FILE: app/config.py
# ==============================================================================
# UPDATED: Added Microsoft Graph API credentials.
# ==============================================================================
import os
import logging
from dotenv import load_dotenv

load_dotenv()

def get_static_config():
    """Loads global configuration from environment variables."""
    logging.info("Loading static configuration from .env file...")
    config = {}
    config['SCHEDULE_MINUTES'] = int(os.getenv("SCHEDULE_MINUTES", 5))
    config['MAX_WORKER_THREADS'] = int(os.getenv("MAX_WORKER_THREADS", 10))
    config['INITIAL_LOOKBACK_DAYS'] = int(os.getenv("INITIAL_LOOKBACK_DAYS", 1))
    config['REMINDER_THRESHOLD_HOURS'] = int(os.getenv("REMINDER_THRESHOLD_HOURS", 24))
    config['OLLAMA_HOST'] = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    config['OLLAMA_MODEL'] = os.getenv("OLLAMA_MODEL", "llama3:8b")
    # MS Graph API credentials
    config['AZURE_TENANT_ID'] = os.getenv("AZURE_TENANT_ID")
    config['AZURE_CLIENT_ID'] = os.getenv("AZURE_CLIENT_ID")
    config['AZURE_CLIENT_SECRET'] = os.getenv("AZURE_CLIENT_SECRET")
    logging.info("Static configuration loaded successfully.")
    return config

# ==============================================================================
# FILE: app/main.py
# ==============================================================================
# This file is unchanged.
# ==============================================================================
import logging
from app.services.email_service import process_mailbox, process_pending_reminders

def run_onboarding_process(config: dict):
    config_id = config.get('config_id', 'unknown')
    logging.info(f"--- [{config_id}] Starting job cycle ---")
    
    logging.info(f"--- [{config_id}] Step 1: Checking for new emails...")
    try:
        process_mailbox(config)
        logging.info(f"--- [{config_id}] Finished processing mailbox.")
    except Exception as e:
        logging.error(f"--- [{config_id}] An unexpected error occurred during mailbox processing: {e}", exc_info=True)
        
    logging.info(f"--- [{config_id}] Step 2: Checking for pending requests...")
    try:
        process_pending_reminders(config)
        logging.info(f"--- [{config_id}] Finished processing pending reminders.")
    except Exception as e:
        logging.error(f"--- [{config_id}] An unexpected error occurred during reminder processing: {e}", exc_info=True)

    logging.info(f"--- [{config_id}] Job cycle finished ---")

# ==============================================================================
# FILE: app/services/ad_service.py
# ==============================================================================
# NEW: This service handles all Microsoft Graph API interactions.
# ==============================================================================
import logging
import msal
import requests

GRAPH_API_ENDPOINT = 'https://graph.microsoft.com/v1.0'
_token_cache = {} # Simple in-memory cache for the access token

def get_access_token(config):
    """Acquires an access token for the Microsoft Graph API."""
    tenant_id = config['AZURE_TENANT_ID']
    client_id = config['AZURE_CLIENT_ID']
    
    if not all([tenant_id, client_id, config.get('AZURE_CLIENT_SECRET')]):
        logging.error("Azure AD credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) are not configured in .env file.")
        return None

    # Check cache first
    if client_id in _token_cache:
        token_result = _token_cache[client_id]
        if "access_token" in token_result and not msal.oauth2cli.oidc.is_token_expired(token_result):
            return token_result.get("access_token")

    authority = f"https://login.microsoftonline.com/{tenant_id}"
    app = msal.ConfidentialClientApplication(
        client_id=client_id,
        authority=authority,
        client_credential=config['AZURE_CLIENT_SECRET'],
    )
    
    token_result = app.acquire_token_for_client(scopes=["https://graph.microsoft.com/.default"])
    _token_cache[client_id] = token_result

    if "access_token" in token_result:
        return token_result["access_token"]
    else:
        logging.error(f"Failed to acquire Graph API token: {token_result.get('error_description')}")
        return None

def get_user_manager(user_email, config):
    """Fetches the line manager's email for a given user from Azure AD."""
    access_token = get_access_token(config)
    if not access_token: return None

    headers = {'Authorization': f'Bearer {access_token}'}
    response = requests.get(f"{GRAPH_API_ENDPOINT}/users/{user_email}/manager", headers=headers)
    
    if response.status_code == 200:
        manager_data = response.json()
        manager_email = manager_data.get('mail')
        if manager_email:
            logging.info(f"Found manager for {user_email}: {manager_email}")
            return manager_email.lower()
    
    logging.warning(f"Could not fetch manager for {user_email}. Status: {response.status_code}, Body: {response.text}")
    return None

def is_user_in_group(user_email, group_name, config):
    """Checks if a user is a member of a specific Azure AD group."""
    access_token = get_access_token(config)
    if not access_token: return False

    headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}
    
    # First, get the group ID from the group name
    group_query = f"$filter=displayName eq '{group_name}'"
    group_response = requests.get(f"{GRAPH_API_ENDPOINT}/groups?{group_query}", headers=headers)
    if group_response.status_code != 200 or not group_response.json().get('value'):
        logging.error(f"AD group '{group_name}' not found.")
        return False
    group_id = group_response.json()['value'][0]['id']

    # Now, check membership
    member_response = requests.get(f"{GRAPH_API_ENDPOINT}/users/{user_email}/memberOf", headers=headers)
    if member_response.status_code == 200:
        member_groups = member_response.json().get('value', [])
        for group in member_groups:
            if group.get('id') == group_id:
                logging.info(f"Confirmed: {user_email} is a member of '{group_name}'.")
                return True
    
    logging.warning(f"User {user_email} is not a member of '{group_name}'.")
    return False

# ==============================================================================
# FILE: app/services/email_service.py
# ==============================================================================
# UPDATED: Integrated AD validation into the new request handling.
# ==============================================================================
import imaplib
import email
from email.header import decode_header
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import logging
import re
from datetime import datetime

from app.services import db_service, ad_service
from app.services.ai_service import analyze_email

def process_mailbox(config):
    """Connects to a team-specific mailbox and processes emails based on their workflow."""
    config_id = config['config_id']
    last_check_timestamp = db_service.get_last_check_time(config_id, config)
    
    try:
        mail = imaplib.IMAP4_SSL(config['imap_server'])
        mail.login(config['imap_user'], config['imap_pass'])
        mail.select("inbox")

        search_criteria = build_search_query(last_check_timestamp, config)
        status, messages = mail.uid('search', None, search_criteria)
        if status != "OK":
            logging.error(f"[{config_id}] Failed to search for emails."); return

        all_uids = messages[0].split()
        if not all_uids:
            logging.info(f"[{config_id}] No new relevant emails found.")
        else:
            processed_uids = db_service.get_processed_uids(all_uids)
            new_uids = [uid for uid in all_uids if uid.decode() not in processed_uids]
            
            for uid in new_uids:
                try:
                    status, msg_data = mail.uid('fetch', uid, "(RFC822)")
                    if status != "OK": continue

                    msg = email.message_from_bytes(msg_data[0][1])
                    subject, _ = decode_header(msg["Subject"])[0]
                    if isinstance(subject, bytes): subject = subject.decode()
                    from_ = msg.get("From")
                    body = get_email_body(msg)
                    thread_id = msg.get('References', msg.get('In-Reply-To', msg.get('Message-ID'))).strip()

                    analysis = analyze_email(subject, body, config)
                    if not analysis:
                        db_service.mark_uid_as_processed(uid.decode()); continue
                        
                    intent = analysis.get('intent')
                    
                    if intent == 'new_request':
                        handle_new_request(from_, analysis, thread_id, config)

                    elif intent == 'approval':
                        handle_approval(from_, thread_id, config)

                    db_service.mark_uid_as_processed(uid.decode())
                except Exception as e:
                    logging.error(f"[{config_id}] Error processing email UID {uid.decode()}: {e}", exc_info=True)
        
        mail.logout()
        db_service.update_last_check_time(config_id, datetime.now().isoformat())
    except Exception as e:
        logging.error(f"[{config_id}] Failed to connect to or process mailbox: {e}", exc_info=True)

def handle_new_request(from_email, analysis, thread_id, config):
    """Validates and processes a new onboarding request with AD checks."""
    config_id = config['config_id']
    user_email = analysis.get('user_email')
    sender_email = (re.search(r'<(.+?)>', from_email) or re.search(r'[\w\.-]+@[\w\.-]+', from_email)).group(0).strip('<>')

    if not user_email:
        logging.warning(f"[{config_id}] New request received but no user email was extracted.")
        return

    # 1. Check for existing access in our DB
    if db_service.check_existing_access(user_email, config_id):
        logging.info(f"[{config_id}] User {user_email} already has access. Sending notification.")
        send_already_onboarded_email(user_email, sender_email, config)
        return

    # 2. AD Group Validation
    required_group = config.get('required_ad_group')
    if required_group and not ad_service.is_user_in_group(user_email, required_group, config):
        rejection_reason = f"User '{user_email}' is not a member of the required AD group '{required_group}' for this system."
        logging.warning(f"[{config_id}] {rejection_reason}")
        send_rejection_email(sender_email, user_email, rejection_reason, config)
        return

    # 3. Line Manager Validation
    line_manager_email = ad_service.get_user_manager(user_email, config)
    if not line_manager_email:
        rejection_reason = f"Could not verify the line manager for '{user_email}' in Active Directory."
        logging.warning(f"[{config_id}] {rejection_reason}")
        send_rejection_email(sender_email, user_email, rejection_reason, config)
        return
        
    if sender_email.lower() != line_manager_email:
        rejection_reason = f"The request was sent by '{sender_email}', but the designated line manager for '{user_email}' is '{line_manager_email}'. The request must be initiated by the user's line manager."
        logging.warning(f"[{config_id}] {rejection_reason}")
        send_rejection_email(sender_email, user_email, rejection_reason, config)
        return

    # All checks passed, create the request
    logging.info(f"[{config_id}] All validations passed for user {user_email}. Creating onboarding request.")
    db_service.create_onboarding_request(thread_id, user_email, config)
    request = db_service.get_request_by_thread_id(thread_id)
    send_request_to_next_stage(request, config)

def handle_approval(from_email, thread_id, config):
    config_id = config['config_id']
    request = db_service.get_request_by_thread_id(thread_id)
    if not request:
        logging.warning(f"[{config_id}] Received an approval for an unknown thread ID: {thread_id}. Skipping.")
        return
    approver_email = (re.search(r'<(.+?)>', from_email) or re.search(r'[\w\.-]+@[\w\.-]+', from_email)).group(0).strip('<>')
    is_stage_complete = db_service.add_stage_approval(request, approver_email, config)
    if is_stage_complete:
        user_to_onboard = request['user_to_onboard_email']
        is_final_stage = request['current_stage'] >= len(config['approval_stages'])
        if is_final_stage:
            logging.info(f"[{config_id}] Final approval received for {user_to_onboard}. Granting access.")
            db_service.update_user_access(user_to_onboard, config_id)
            db_service.update_request_status(thread_id, 'completed', f"Access granted for {user_to_onboard}.")
            send_confirmation_email(user_to_onboard, thread_id, config)
        else:
            logging.info(f"[{config_id}] Stage {request['current_stage']} complete for {user_to_onboard}. Advancing to next stage.")
            advanced_request = db_service.advance_to_next_stage(thread_id)
            send_request_to_next_stage(advanced_request, config)

def send_request_to_next_stage(request, config):
    config_id = config['config_id']
    current_stage_num = request['current_stage']
    stage_info = next((stage for stage in config['approval_stages'] if stage['stage'] == current_stage_num), None)
    if not stage_info:
        logging.error(f"[{config_id}] Could not find stage info for stage {current_stage_num} in config.")
        return
    user_to_onboard = request['user_to_onboard_email']
    thread_id = request['request_thread_id']
    next_approvers = stage_info['approvers']
    stage_name = stage_info.get('name', f"Stage {current_stage_num}")
    logging.info(f"[{config_id}] Sending request for '{stage_name}' approval to: {next_approvers}")
    subject = f"ACTION REQUIRED: {stage_name} Approval for Onboarding {user_to_onboard}"
    body = f"Hello,\n\nThis onboarding request for user '{user_to_onboard}' requires your approval for '{stage_name}'.\n\nPlease reply to this email with 'Approved' or 'Rejected'.\n\nThank you,\n{config['team_alias']}"
    send_email(next_approvers, subject, body, config)
    db_service.update_request_status(thread_id, f"pending_stage_{current_stage_num}_approval", f"Request sent to {stage_name} approvers.")

def send_email(recipients, subject, body, config):
    msg = MIMEMultipart()
    msg['From'] = f"{config['team_alias']} <{config['smtp_user']}>"
    msg['To'] = ", ".join(recipients)
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'plain'))
    try:
        with smtplib.SMTP(config['smtp_server'], config['smtp_port']) as server:
            server.starttls()
            server.login(config['smtp_user'], config['smtp_pass'])
            server.send_message(msg)
            logging.info(f"Successfully sent email with subject '{subject}' to {recipients}")
    except Exception as e:
        logging.error(f"Failed to send email: {e}", exc_info=True)

def send_confirmation_email(user_email, thread_id, config):
    subject = f"Welcome! Your Access to {config['team_alias']} is Granted"
    body = f"Hello,\n\nThis is an automated message to confirm that your account ({user_email}) has been successfully onboarded and access has been granted.\n\nBest regards,\n{config['team_alias']}\n\n(Reference ID: {thread_id})"
    send_email([user_email], subject, body, config)

def send_already_onboarded_email(user_email, sender_email, config):
    subject = f"Access Notification for {config['team_alias']}"
    body = f"Hello,\n\nThis is an automated notification from the {config['team_alias']}.\n\nOur records indicate that the user '{user_email}' already has an active account for this system. If they are having trouble accessing their account, please use the 'Forgot Password' or account recovery options.\n\nNo further action is needed on this request.\n\nBest regards,\n{config['team_alias']}"
    send_email([sender_email], subject, body, config)

def send_rejection_email(recipient, rejected_user, reason, config):
    """Sends an automated rejection email."""
    subject = f"Onboarding Request for {rejected_user} - Rejected"
    body = f"Hello,\n\nThis is an automated notification from the {config['team_alias']}.\n\nThe onboarding request for user '{rejected_user}' has been automatically rejected for the following reason:\n\n{reason}\n\nPlease correct the issue and resubmit the request.\n\nBest regards,\n{config['team_alias']}"
    send_email([recipient], subject, body, config)

def process_pending_reminders(config):
    config_id = config['config_id']
    pending_requests = db_service.get_pending_requests_for_reminder(config)
    if not pending_requests:
        logging.info(f"[{config_id}] No pending requests found that require a reminder.")
        return
    logging.info(f"[{config_id}] Found {len(pending_requests)} pending request(s) requiring a reminder.")
    for request in pending_requests:
        try:
            send_reminder_email(request, config)
        except Exception as e:
            logging.error(f"[{config_id}] Failed to send reminder for request ID {request['id']}: {e}", exc_info=True)

def send_reminder_email(request, config):
    user_to_onboard = request['user_to_onboard_email']
    missing_approvers = db_service.get_missing_approvers_for_stage(request, config)
    if not missing_approvers:
        logging.warning(f"Request ID {request['id']} is pending but has no missing approvers for the current stage. Skipping reminder.")
        return
    logging.info(f"Preparing to send reminder for {user_to_onboard} to: {', '.join(missing_approvers)}")
    subject = f"REMINDER: Approval Required for Onboarding {user_to_onboard}"
    body = f"Hello,\n\nThis is an automated reminder that your approval is required for the user onboarding request for:\n\nUser: {user_to_onboard}\n\nThis request is currently pending your action. Please reply to the original request with your approval.\n\nThank you,\n{config['team_alias']}\n\n(Original Reference ID: {request['request_thread_id']})"
    send_email(missing_approvers, subject, body, config)
    db_service.update_request_status(request['request_thread_id'], request['status'], f"Reminder sent to: {', '.join(missing_approvers)}")

def build_search_query(last_check_timestamp, config):
    search_date = datetime.fromisoformat(last_check_timestamp).strftime('%d-%b-%Y')
    criteria = [f'(SINCE "{search_date}")']
    all_approvers = set()
    if config.get('approval_stages'):
        for stage in config['approval_stages']: all_approvers.update(stage.get('approvers', []))
    addresses_to_check = list(all_approvers) + [config.get('imap_user')]
    address_criteria = [f'(OR (TO "{addr}") (CC "{addr}"))' for addr in addresses_to_check if addr]
    if address_criteria: criteria.append(f"(OR {' '.join(address_criteria)})")
    return " ".join(criteria)

def get_email_body(msg):
    if msg.is_multipart():
        for part in msg.walk():
            if part.get_content_type() == "text/plain" and "attachment" not in str(part.get("Content-Disposition")):
                try: return part.get_payload(decode=True).decode()
                except: return part.get_payload(decode=True).decode('latin-1')
    else:
        try: return msg.get_payload(decode=True).decode()
        except: return msg.get_payload(decode=True).decode('latin-1')
    return ""

# ==============================================================================
# FILE: app/services/db_service.py
# ==============================================================================
# UPDATED: Complete code with new schemas and logic for multi-stage workflows.
# ==============================================================================
import psycopg2
import logging
import json
import os
from psycopg2.extras import DictCursor
from datetime import datetime, timedelta

def get_db_connection():
    """Establishes a connection to the PostgreSQL database."""
    try:
        conn = psycopg2.connect(
            dbname=os.getenv("DB_NAME"), user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASS"), host=os.getenv("DB_HOST"),
            port=os.getenv("DB_PORT")
        )
        return conn
    except psycopg2.OperationalError as e:
        logging.error(f"Could not connect to PostgreSQL database: {e}"); raise

def setup_database():
    """Creates all necessary tables with the new schema."""
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            cur.execute("DROP TABLE IF EXISTS users;")
            cur.execute("""
                CREATE TABLE users (
                    email TEXT NOT NULL,
                    config_id TEXT NOT NULL,
                    access_flag BOOLEAN DEFAULT FALSE,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (email, config_id)
                );
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS user_directory (
                    email TEXT PRIMARY KEY,
                    full_name TEXT,
                    line_manager_email TEXT,
                    ad_groups JSONB DEFAULT '[]'::jsonb
                );
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS onboarding_tracker (
                    id SERIAL PRIMARY KEY,
                    request_thread_id TEXT UNIQUE NOT NULL,
                    user_to_onboard_email VARCHAR(255),
                    config_id TEXT,
                    status VARCHAR(100) NOT NULL DEFAULT 'new',
                    current_stage INT DEFAULT 1,
                    stage_approvals JSONB DEFAULT '{}'::jsonb,
                    delegated_approvers JSONB DEFAULT '[]'::jsonb,
                    last_activity_details TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            cur.execute("CREATE TABLE IF NOT EXISTS app_state (key TEXT PRIMARY KEY, value TEXT NOT NULL);")
            cur.execute("CREATE TABLE IF NOT EXISTS processed_uids (uid TEXT PRIMARY KEY, processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP);")
            cur.execute("DROP TABLE IF EXISTS configuration;")
            cur.execute("""
                CREATE TABLE configuration (
                    config_id TEXT PRIMARY KEY,
                    description TEXT,
                    is_active BOOLEAN DEFAULT TRUE,
                    team_alias TEXT NOT NULL,
                    imap_server TEXT NOT NULL,
                    imap_user TEXT NOT NULL,
                    imap_pass TEXT NOT NULL,
                    smtp_server TEXT NOT NULL,
                    smtp_port INT NOT NULL,
                    smtp_user TEXT NOT NULL,
                    smtp_pass TEXT NOT NULL,
                    workflow_type TEXT NOT NULL DEFAULT 'simple',
                    approval_stages JSONB,
                    required_ad_group TEXT
                );
            """)
            cur.execute("CREATE OR REPLACE FUNCTION trigger_set_timestamp() RETURNS TRIGGER AS $$ BEGIN NEW.updated_at = NOW(); RETURN NEW; END; $$ LANGUAGE plpgsql;")
            cur.execute("DROP TRIGGER IF EXISTS set_timestamp ON onboarding_tracker; CREATE TRIGGER set_timestamp BEFORE UPDATE ON onboarding_tracker FOR EACH ROW EXECUTE PROCEDURE trigger_set_timestamp();")
            conn.commit()
            logging.info("Database tables verified/created successfully.")
    except Exception as e: logging.error(f"Database setup failed: {e}")
    finally:
        if conn: conn.close()

def get_all_active_configurations():
    """Loads all active configuration profiles from the database."""
    conn = get_db_connection()
    configs = []
    try:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("SELECT * FROM configuration WHERE is_active = TRUE;")
            for row in cur.fetchall():
                configs.append(dict(row))
    except Exception as e:
        logging.error(f"Could not load active configurations from database: {e}", exc_info=True)
    finally:
        if conn: conn.close()
    return configs

def get_missing_approvers_for_stage(request, config):
    """Calculates which approvers for the current stage have not yet approved."""
    current_stage_num = request['current_stage']
    stage_info = next((s for s in config['approval_stages'] if s['stage'] == current_stage_num), None)
    if not stage_info: return []
    
    required_approvers = set(stage_info['approvers'])
    received_approvals = set(request['stage_approvals'].get(str(current_stage_num), []))
    
    return list(required_approvers - received_approvals)

def add_stage_approval(request, approver_email, config):
    """Adds an approval for the current stage and checks if the stage is complete."""
    conn = get_db_connection()
    try:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            current_stage_num = request['current_stage']
            current_stage_str = str(current_stage_num)
            
            cur.execute("""
                UPDATE onboarding_tracker
                SET stage_approvals = jsonb_set(
                    stage_approvals, ARRAY[%s],
                    (COALESCE(stage_approvals->%s, '[]'::jsonb) || %s::jsonb) - %s || %s::jsonb
                ),
                last_activity_details = %s
                WHERE request_thread_id = %s RETURNING *;
            """, (current_stage_str, current_stage_str, json.dumps(approver_email), json.dumps(approver_email), json.dumps(approver_email), f"Approval for stage {current_stage_str} received from {approver_email}", request['request_thread_id']))
            
            updated_request = cur.fetchone()
            conn.commit()

            if not updated_request: return False

            stage_info = next((s for s in config['approval_stages'] if s['stage'] == current_stage_num), None)
            if not stage_info: return False

            required_approvers = set(stage_info['approvers'])
            received_approvals = set(updated_request['stage_approvals'].get(current_stage_str, []))
            
            return required_approvers.issubset(received_approvals)
    except Exception as e:
        logging.error(f"Failed to add stage approval: {e}", exc_info=True)
        return False
    finally:
        if conn: conn.close()

def advance_to_next_stage(thread_id):
    """Increments the stage counter for a request."""
    conn = get_db_connection()
    try:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("UPDATE onboarding_tracker SET current_stage = current_stage + 1 WHERE request_thread_id = %s RETURNING *;", (thread_id,))
            request = cur.fetchone()
            conn.commit()
            return request
    finally:
        if conn: conn.close()

def check_existing_access(user_email, config_id):
    """Checks if a user already has access for a specific system."""
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT access_flag FROM users WHERE email = %s AND config_id = %s;", (user_email, config_id))
            result = cur.fetchone()
            return result[0] if result else False
    except Exception as e:
        logging.error(f"Failed to check existing access for {user_email}: {e}", exc_info=True)
        return False
    finally:
        if conn: conn.close()

def create_onboarding_request(thread_id, user_email, config):
    """Creates a new record in the tracker, linking it to a config."""
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            cur.execute("INSERT INTO onboarding_tracker (request_thread_id, user_to_onboard_email, config_id, status, last_activity_details) VALUES (%s, %s, %s, %s, %s) ON CONFLICT (request_thread_id) DO NOTHING;", (thread_id, user_email, config['config_id'], f"pending_stage_1_approval", f"New onboarding request received for {user_email}."))
            conn.commit()
            if cur.rowcount > 0: logging.info(f"New onboarding request created for {user_email} with thread ID {thread_id[:15]}...")
            else: logging.info(f"Onboarding request for thread ID {thread_id[:15]}... already exists.")
    except Exception as e: logging.error(f"Failed to create onboarding request: {e}", exc_info=True)
    finally:
        if conn: conn.close()

def update_user_access(user_email, config_id):
    """Grants access to a user for a specific system."""
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            cur.execute("""
                INSERT INTO users (email, config_id, access_flag)
                VALUES (%s, %s, TRUE)
                ON CONFLICT (email, config_id) DO UPDATE SET access_flag = TRUE;
            """, (user_email, config_id))
            conn.commit()
            logging.info(f"Successfully granted access for user {user_email} to system {config_id}")
    except Exception as e:
        logging.error(f"Failed to update user access for {user_email}: {e}", exc_info=True)
    finally:
        if conn: conn.close()

def get_last_check_time(config_id, config):
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            state_key = f"last_check_timestamp_{config_id}"
            cur.execute("SELECT value FROM app_state WHERE key = %s;", (state_key,))
            result = cur.fetchone()
            if result: return result[0]
            else:
                initial_time = (datetime.now() - timedelta(days=config['INITIAL_LOOKBACK_DAYS'])).isoformat()
                logging.info(f"[{config_id}] No last check timestamp found in DB. Using initial lookback: {initial_time}")
                return initial_time
    except Exception as e:
        logging.error(f"Failed to get last check time for {config_id}: {e}", exc_info=True)
        return (datetime.now() - timedelta(days=1)).isoformat()
    finally:
        if conn: conn.close()

def update_last_check_time(config_id, timestamp_iso):
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            state_key = f"last_check_timestamp_{config_id}"
            cur.execute("INSERT INTO app_state (key, value) VALUES (%s, %s) ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value;", (state_key, timestamp_iso))
            conn.commit()
            logging.info(f"[{config_id}] Updated last check timestamp to: {timestamp_iso}")
    except Exception as e: logging.error(f"Failed to update last check time for {config_id}: {e}", exc_info=True)
    finally:
        if conn: conn.close()

def get_pending_requests_for_reminder(config):
    conn = get_db_connection()
    try:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            query = "SELECT * FROM onboarding_tracker WHERE status LIKE 'pending_stage%%' AND updated_at < NOW() - INTERVAL '%s hours'"
            cur.execute(query, (config['REMINDER_THRESHOLD_HOURS'],))
            return cur.fetchall()
    except Exception as e:
        logging.error(f"Failed to get pending requests: {e}", exc_info=True)
        return []
    finally:
        if conn: conn.close()

def get_request_by_thread_id(thread_id):
    conn = get_db_connection()
    try:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("SELECT * FROM onboarding_tracker WHERE request_thread_id = %s;", (thread_id,))
            return cur.fetchone()
    except Exception as e:
        logging.error(f"Failed to get request for thread {thread_id}: {e}", exc_info=True)
        return None
    finally:
        if conn: conn.close()

def update_request_status(thread_id, status, details):
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            cur.execute("UPDATE onboarding_tracker SET status = %s, last_activity_details = %s WHERE request_thread_id = %s;", (status, details, thread_id))
            conn.commit()
            logging.info(f"Updated status for thread {thread_id[:15]}... to '{status}'")
    except Exception as e: logging.error(f"Failed to update request status: {e}", exc_info=True)
    finally:
        if conn: conn.close()

def get_processed_uids(uids_to_check):
    if not uids_to_check: return set()
    uids_to_check_str = [uid.decode() if isinstance(uid, bytes) else uid for uid in uids_to_check]
    conn = get_db_connection()
    processed_uids = set()
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT uid FROM processed_uids WHERE uid = ANY(%s);", (uids_to_check_str,))
            for row in cur.fetchall(): processed_uids.add(row[0])
    except Exception as e: logging.error(f"Failed to get processed UIDs: {e}", exc_info=True)
    finally:
        if conn: conn.close()
    return processed_uids

def mark_uid_as_processed(uid):
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            cur.execute("INSERT INTO processed_uids (uid) VALUES (%s) ON CONFLICT (uid) DO NOTHING;", (uid,))
            conn.commit()
    except Exception as e: logging.error(f"Failed to mark UID {uid} as processed: {e}", exc_info=True)
    finally:
        if conn: conn.close()

# ==============================================================================
# FILE: app/services/ai_service.py
# ==============================================================================
# This file is unchanged.
# ==============================================================================
import logging
import json
import re
import ollama

client = None

def analyze_email(subject, body, config):
    """Uses a local Ollama model to classify email intent and extract entities."""
    global client
    if client is None:
        logging.info(f"Initializing Ollama client with host: {config['OLLAMA_HOST']}")
        client = ollama.Client(host=config['OLLAMA_HOST'])

    full_content = f"Subject: {subject}\n\nBody:\n{body}"
    full_content = re.sub(r'\s+', ' ', full_content).strip()[:4000]
    
    system_prompt = """
    You are an intelligent assistant for an IT support desk. Your task is to analyze incoming emails and classify their intent.
    You must respond ONLY with a valid JSON object and nothing else.

    Possible intents are:
    - 'new_request': An initial request to onboard a new user. Keywords: 'onboard', 'new user', 'access request'.
    - 'approval': An approval for a previous request. Keywords: 'approved', 'approve', 'ok to proceed'.
    - 'rejection': A rejection of a request. Keywords: 'rejected', 'denied'.
    - 'out_of_office': An automated out-of-office or vacation reply. Keywords: 'out of office', 'on vacation', 'limited access to email'.
    - 'query': A question or unrelated to onboarding.

    JSON Response Format:
    - For 'new_request', respond with: {"intent": "new_request", "user_email": "email@example.com"}
    - For 'out_of_office', respond with: {"intent": "out_of_office", "delegate_email": "delegate@example.com"}. If no delegate is mentioned, delegate_email must be null.
    - For all other intents, respond with: {"intent": "your_intent", "user_email": null, "delegate_email": null}
    """
    try:
        response = client.chat(
            model=config['OLLAMA_MODEL'],
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': full_content}
            ],
            options={'temperature': 0.0}
        )
        result_json_str = response['message']['content']
        cleaned_json_str = re.sub(r'^```json\s*|\s*```$', '', result_json_str, flags=re.MULTILINE).strip()
        logging.info(f"AI Analysis Result: {cleaned_json_str}")
        return json.loads(cleaned_json_str)
    except Exception as e:
        logging.error(f"Error calling Ollama: {e}", exc_info=True)
        return None
