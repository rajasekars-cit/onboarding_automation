# FILE: run.py
# ==============================================================================
# Orchestrates the onboarding workflow using a producer-consumer model.
# The producer groups configurations by mailbox and creates a polling task
# for each unique mailbox, supporting both shared and independent mailboxes.
# ==============================================================================

import time
import logging
import queue
import threading
from concurrent.futures import ThreadPoolExecutor
from app.main import run
from app.services import db_service
from app.config import get_static_config
from collections import defaultdict

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(threadName)s - %(module)s - %(message)s'
)

WORK_QUEUE = queue.Queue()

def worker_thread(static_config):
    """ Worker thread that dequeues and processes one mailbox task at a time. """
    logging.info("Worker thread started")
    while True:
        task = WORK_QUEUE.get()
        if task is None:
            logging.info("Worker received sentinel to shut down")
            break
        try:
            mailbox_id = task.get('mailbox_config', {}).get('id')
            logging.info(f"Worker picking up task for mailbox_id: {mailbox_id}")
            run(task['mailbox_config'], task['associated_configs'], static_config)
        except Exception as e:
            mailbox_id = task.get('mailbox_config', {}).get('id', 'unknown')
            logging.error(f"Error in worker for mailbox {mailbox_id}: {e}", exc_info=True)
        finally:
            WORK_QUEUE.task_done()

def producer_thread():
    """ Producer thread that groups configs by mailbox and schedules one task per mailbox. """
    while True:
        try:
            logging.info("Producer waking to schedule tasks")
            
            # Group configurations by their mailbox_id to create one task per mailbox
            active_configs = db_service.get_all_active_configurations()
            tasks_by_mailbox = defaultdict(list)
            for config in active_configs:
                if config.get('mailbox_id'):
                    tasks_by_mailbox[config['mailbox_id']].append(config)

            if not tasks_by_mailbox:
                logging.info("No active mailboxes to poll.")
            else:
                logging.info(f"Scheduling polling for {len(tasks_by_mailbox)} unique mailboxes.")
                for mailbox_id, associated_configs in tasks_by_mailbox.items():
                    mailbox_config = db_service.get_mailbox_config_by_id(mailbox_id)
                    if mailbox_config:
                        task = {
                            "mailbox_config": mailbox_config,
                            "associated_configs": associated_configs
                        }
                        WORK_QUEUE.put(task)
                    else:
                        logging.error(f"Could not find mailbox configuration for mailbox_id: {mailbox_id}. Skipping.")

            static_config = get_static_config()
            interval = static_config.get("SCHEDULE_MINUTES", 5) * 60
            time.sleep(interval)
        except Exception as e:
            logging.error(f"Producer thread error: {e}", exc_info=True)
            time.sleep(60)

if __name__ == "__main__":
    logging.info("Starting onboarding application with thread pool")
    static_cfg = get_static_config()
    max_workers = static_cfg.get("MAX_WORKER_THREADS", 10)

    db_service.setup_database()

    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="Worker") as executor:
        for _ in range(max_workers):
            executor.submit(worker_thread, static_cfg)

        producer = threading.Thread(target=producer_thread, name="Producer")
        producer.daemon = True
        producer.start()

        try:
            producer.join()
        except KeyboardInterrupt:
            logging.info("Shutdown requested, signaling workers to stop...")
            for _ in range(max_workers):
                WORK_QUEUE.put(None)

# FILE: app/config.py
# ==============================================================================
# Static configuration loader for environment variables
# ==============================================================================

import os
import logging
from dotenv import load_dotenv

# Load environment variables from a .env file if it exists
load_dotenv()

def get_static_config():
    """Loads static configuration from environment variables."""
    logging.info("Loading static configuration from environment variables")
    config = {}
    config['SCHEDULE_MINUTES'] = int(os.getenv("SCHEDULE_MINUTES", 1))
    config['MAX_WORKER_THREADS'] = int(os.getenv("MAX_WORKER_THREADS", 5))
    config['INITIAL_LOOKBACK_DAYS'] = int(os.getenv("INITIAL_LOOKBACK_DAYS", 1))
    config['REMINDER_THRESHOLD_HOURS'] = int(os.getenv("REMINDER_THRESHOLD_HOURS", 24))
    
    # NEW: Delay for the action worker to process staged requests
    config['MATURITY_DELAY_MINUTES'] = int(os.getenv("MATURITY_DELAY_MINUTES", 2))

    # AI Service Config
    config['OLLAMA_HOST'] = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    config['OLLAMA_MODEL'] = os.getenv("OLLAMA_MODEL", "llama3:8b")
    
    # Azure AD / Graph API Config
    config['AZURE_TENANT_ID'] = os.getenv("AZURE_TENANT_ID")
    config['AZURE_CLIENT_ID'] = os.getenv("AZURE_CLIENT_ID")
    config['AZURE_CLIENT_SECRET'] = os.getenv("AZURE_CLIENT_SECRET")
    
    logging.info("Static configuration loaded.")
    return config

# FILE: app/main.py
# ==============================================================================
# Main workflow orchestrator called by worker threads.
# It manages the two-phase workflow for a single mailbox and all its
# associated configurations (e.g., one shared mailbox for DEV and DBA).
# ==============================================================================

import logging
from app.services.email_service import (
    ingest_emails_to_db, 
    process_pending_actions, 
    process_pending_reminders
)

def run(mailbox_config, associated_configs, static_config):
    """
    Orchestrates the workflow for a single mailbox and its associated configurations.
    """
    mailbox_id = mailbox_config.get('id')
    logging.info(f"[Mailbox ID: {mailbox_id}] Starting two-phase workflow cycle.")

    # Phase 1: Ingest emails from the single mailbox
    try:
        logging.info(f"[Mailbox ID: {mailbox_id}] --- Phase 1: Ingesting Emails ---")
        ingest_emails_to_db(mailbox_config, associated_configs, static_config)
    except Exception as e:
        logging.error(f"[Mailbox ID: {mailbox_id}] Error during email ingestion phase: {e}", exc_info=True)

    # Phase 2: Process actions for each configuration associated with this mailbox
    logging.info(f"[Mailbox ID: {mailbox_id}] --- Phase 2: Processing Actions & Reminders ---")
    for dynamic_config in associated_configs:
        config_id = dynamic_config.get('config_id')
        try:
            # Create a full, merged config object for each specific workflow
            full_config = {**static_config, **mailbox_config, **dynamic_config}
            process_pending_actions(full_config)
            process_pending_reminders(full_config)
        except Exception as e:
            logging.error(f"[Config ID: {config_id}] Error during action/reminder phase: {e}", exc_info=True)

    logging.info(f"[Mailbox ID: {mailbox_id}] Completed two-phase workflow cycle")

# FILE: app/services/ad_service.py
# ==============================================================================
# Service for interacting with Azure Active Directory via Microsoft Graph API.
# Handles token acquisition, user/group lookups, and membership checks.
# ==============================================================================
import logging
import msal
import requests

GRAPH_API_ENDPOINT = 'https://graph.microsoft.com/v1.0'
_app_cache = {} # Cache the MSAL app object to reuse its internal token cache

def get_access_token(config):
    """Acquires an access token for the Microsoft Graph API, using MSAL's built-in caching."""
    tenant_id = config['AZURE_TENANT_ID']
    client_id = config['AZURE_CLIENT_ID']
    
    if not all([tenant_id, client_id, config.get('AZURE_CLIENT_SECRET')]):
        logging.error("Azure AD credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) are not configured.")
        return None

    # Reuse the app object to leverage MSAL's internal token cache
    if client_id not in _app_cache:
        authority = f"https://login.microsoftonline.com/{tenant_id}"
        app = msal.ConfidentialClientApplication(
            client_id=client_id,
            authority=authority,
            client_credential=config['AZURE_CLIENT_SECRET'],
        )
        _app_cache[client_id] = app
    
    app = _app_cache[client_id]
    
    # acquire_token_for_client will automatically use its cache and refresh if needed.
    token_result = app.acquire_token_for_client(scopes=["https://graph.microsoft.com/.default"])

    if "access_token" in token_result:
        return token_result["access_token"]
    else:
        logging.error(f"Failed to acquire Graph API token: {token_result.get('error_description')}")
        # Clear the app from cache in case of persistent auth failure
        if client_id in _app_cache:
            del _app_cache[client_id]
        return None

def get_user_id(user_email, token):
    """Gets the Azure AD object ID for a user from their email."""
    headers = {"Authorization": f"Bearer {token}"}
    
    # This filter is proven to work for finding both internal and guest users.
    params = {"$filter": f"mail eq '{user_email}' or userPrincipalName eq '{user_email}'"}

    response = requests.get(f"{GRAPH_API_ENDPOINT}/users", headers=headers, params=params)
    if response.status_code == 200:
        data = response.json().get("value")
        if data: 
            # logging.info(f"Found user with ID: {data[0]['id']}")
            return data[0]["id"]
    else:
        logging.error(f"Graph API error when searching for user '{user_email}': {response.status_code} - {response.text}")
        
    return None

def get_group_id(group_name, token):
    """Gets the Azure AD object ID for a group from its display name."""
    headers = {"Authorization": f"Bearer {token}"}
    params = {"$filter": f"displayName eq '{group_name}'"}
    response = requests.get(f"{GRAPH_API_ENDPOINT}/groups", headers=headers, params=params)
    if response.status_code == 200:
        data = response.json().get("value")
        if data: return data[0]["id"]
    return None

def is_user_in_group(user_email, group_name, config):
    """Checks if a user is a member of a specific Azure AD group."""
    token = get_access_token(config)
    if not token: return False
    group_id = get_group_id(group_name, token)
    if not group_id:
        logging.error(f"AD group '{group_name}' not found.")
        return False
    user_id = get_user_id(user_email, token)
    if not user_id:
        logging.warning(f"User '{user_email}' not found in AD for group check.")
        return False
    headers = {"Authorization": f"Bearer {token}"}
    json_payload = {"groupIds": [group_id]}
    response = requests.post(f"{GRAPH_API_ENDPOINT}/users/{user_id}/checkMemberGroups", headers=headers, json=json_payload)
    if response.status_code == 200:
        if group_id in response.json().get("value", []):
            logging.info(f"AD check PASSED: {user_email} is a member of '{group_name}'.")
            return True
    logging.warning(f"AD check FAILED: {user_email} is NOT a member of '{group_name}'.")
    return False

def get_user_manager(user_email, config):
    """Fetches the line manager's email for a given user from Azure AD."""
    token = get_access_token(config)
    if not token: return None
    user_id = get_user_id(user_email, token)
    if not user_id:
        logging.warning(f"Could not find user ID for '{user_email}'. Cannot fetch manager.")
        return None

    headers = {'Authorization': f'Bearer {token}'}
    
    # ===== FIX STARTS HERE =====
    # Step 1: Get the manager object. This might return a limited profile.
    manager_response = requests.get(f"{GRAPH_API_ENDPOINT}/users/{user_id}/manager", headers=headers)

    if manager_response.status_code == 200:
        manager_data = manager_response.json()
        manager_id = manager_data.get('id')
        
        if not manager_id:
            logging.warning(f"Found a manager object for {user_email}, but it has no ID.")
            return None

        # Step 2: Use the manager's ID to get their full user profile, which includes the email.
        full_profile_response = requests.get(f"{GRAPH_API_ENDPOINT}/users/{manager_id}", headers=headers)
        
        if full_profile_response.status_code == 200:
            full_profile_data = full_profile_response.json()
            manager_email = full_profile_data.get('mail')
            if manager_email:
                # logging.info(f"Found manager for {user_email}: {manager_email}")
                return manager_email.lower()
        else:
            logging.warning(f"Could not fetch full profile for manager of {user_email}. Status: {full_profile_response.status_code}, Body: {full_profile_response.text}")

    logging.warning(f"Could not fetch manager for {user_email}. Status: {manager_response.status_code}, Body: {manager_response.text}")
    return None
    # ===== FIX ENDS HERE =====

def get_group_owners(group_name, config):
    """Fetches the email addresses of the owners of a specific AD group."""
    token = get_access_token(config)
    if not token: return []
    group_id = get_group_id(group_name, token)
    if not group_id:
        logging.error(f"Cannot get owners because AD group '{group_name}' was not found.")
        return []
    headers = {'Authorization': f'Bearer {token}'}
    
    # Explicitly select the fields needed to ensure they are returned.
    params = {"$select": "displayName,mail"}
    response = requests.get(f"{GRAPH_API_ENDPOINT}/groups/{group_id}/owners", headers=headers, params=params)
    
    if response.status_code == 200:
        owners_data = response.json().get("value", [])
        owner_emails = [owner.get('mail').lower() for owner in owners_data if owner.get('mail')]
        logging.info(f"Found owners for group '{group_name}': {owner_emails}")
        return owner_emails
    logging.error(f"Error fetching owners for group '{group_name}': {response.text}")
    return []

# FILE: app/services/email_service.py
# ==============================================================================
# Handles all email-related tasks for the two-phase workflow.
# The ingestion function polls a single mailbox (which can be shared by
# multiple configurations) and uses a dispatcher model to route requests
# to the correct workflow based on the content of the email.
# ==============================================================================

import imaplib
import email
from email.header import decode_header
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import logging
import re
from datetime import datetime

from app.services import db_service, ad_service
from app.services.ai_service import analyze_email

# Regular expression for matching automated email addresses to ignore them.
AUTO_ADDRESS_PATTERNS = [
    r'no-?reply@', r'notification', r'do-?not-?reply@', r'mailer-daemon', r'postmaster@',
    r'automated', r'helpdesk', r'bounces@', r'^noreply', r'bot@', r'listserv',
    r'system@', r'alerts?@'
]

# --- Utility Functions ---

def is_autogenerated_address(email_address):
    """Checks if an email address matches common patterns for automated senders."""
    if not email_address:
        return True
    return any(re.search(pat, email_address, re.IGNORECASE) for pat in AUTO_ADDRESS_PATTERNS)

def extract_email(from_header):
    """Extracts a clean email address from a 'From' header string."""
    if not from_header:
        return None
    match = re.search(r'<([^>]+)>', from_header)
    if match:
        return match.group(1).strip().lower()
    match = re.search(r'[\w.\-+]+@[\w.\-]+', from_header)
    if match:
        return match.group(0).strip().lower()
    return None

def get_email_body(msg):
    """Extracts the plain text body from an email message object."""
    if msg.is_multipart():
        for part in msg.walk():
            content_type = part.get_content_type()
            content_disp = str(part.get("Content-Disposition") or "")
            if content_type == "text/plain" and "attachment" not in content_disp.lower():
                try:
                    return part.get_payload(decode=True).decode()
                except (UnicodeDecodeError, AttributeError):
                    return part.get_payload(decode=True).decode('latin-1', errors='ignore')
    else:
        try:
            return msg.get_payload(decode=True).decode()
        except (UnicodeDecodeError, AttributeError):
            return msg.get_payload(decode=True).decode('latin-1', errors='ignore')
    return ""

def build_search_query(last_check_timestamp):
    """Builds the IMAP search query based on the last check timestamp."""
    search_date = datetime.fromisoformat(last_check_timestamp).strftime('%d-%b-%Y')
    return f'(SINCE "{search_date}")'

def send_email(recipients, subject, body, config):
    """Sends an email using the merged config which contains all necessary credentials."""
    msg = MIMEMultipart()
    msg['From'] = f"{config['team_alias']} <{config['smtp_user']}>"
    msg['To'] = ", ".join(recipients)
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'plain'))
    try:
        with smtplib.SMTP(config['smtp_server'], config['smtp_port']) as server:
            server.starttls()
            server.login(config['smtp_user'], config['smtp_pass'])
            server.send_message(msg)
            logging.info(f"Sent email '{subject}' to {recipients}")
    except Exception as e:
        logging.error(f"Failed to send email '{subject}' to {recipients}: {e}", exc_info=True)

# ==============================================================================
# Phase 1: Ingestion Worker (Read & Record)
# ==============================================================================

def ingest_emails_to_db(mailbox_config, associated_configs, static_config):
    """
    PHASE 1: Polls ONE mailbox and dispatches emails to its associated configurations.
    """
    mailbox_id = mailbox_config['id']
    full_mailbox_config = {**static_config, **mailbox_config}
    last_check_timestamp_key = f"MAILBOX_{mailbox_id}"
    last_check_timestamp = db_service.get_last_check_time(last_check_timestamp_key, full_mailbox_config)
    
    group_config_map = {
        (c.get('required_ad_group') or '').upper(): c
        for c in associated_configs
    }

    try:
        with imaplib.IMAP4_SSL(full_mailbox_config['imap_server']) as mail:
            mail.login(full_mailbox_config['imap_user'], full_mailbox_config['imap_pass'])
            mail.select("INBOX")
            search_query = build_search_query(last_check_timestamp)
            status, message_data = mail.uid('search', None, search_query)
            if status != "OK":
                logging.error(f"[Mailbox ID: {mailbox_id}] IMAP search failed.")
                return

            all_uids = message_data[0].split()
            if all_uids:
                logging.info(f"[Mailbox ID: {mailbox_id}] Found {len(all_uids)} new emails to process.")
            
            for uid in all_uids:
                uid_str = uid.decode()
                if not db_service.claim_uid_for_processing(uid_str):
                    continue

                try:
                    fetch_status, msg_parts = mail.uid('fetch', uid, '(RFC822)')
                    if fetch_status != 'OK' or not msg_parts or not msg_parts[0]:
                        logging.error(f"Failed to fetch email content for UID {uid_str}. Status: {fetch_status}.")
                        continue
                    
                    msg = email.message_from_bytes(msg_parts[0][1])
                    subject, _ = decode_header(msg.get("Subject") or "")[0]
                    if isinstance(subject, bytes): subject = subject.decode('utf-8', 'ignore')
                    
                    sender_email = extract_email(msg.get("From"))
                    if not sender_email or is_autogenerated_address(sender_email):
                        continue

                    body = get_email_body(msg)
                    analysis = analyze_email(subject, body, full_mailbox_config)
                    if not analysis:
                        logging.warning(f"AI analysis for UID {uid_str} failed. Skipping.")
                        continue

                    intent = analysis.get('intent')
                    user_email = analysis.get('user_email')

                    if intent == 'new_request':
                        requested_group = (analysis.get('requested_group') or '').upper()
                        target_config = group_config_map.get(requested_group)
                        if target_config:
                            logging.info(f"Staging new request for user '{user_email}' for config '{target_config['config_id']}'.")
                            db_service.create_onboarding_request_composite(
                                user_email=user_email,
                                group=requested_group,
                                config_id=target_config['config_id']
                            )
                        else:
                            logging.warning(f"Request for group '{requested_group}' received in mailbox {mailbox_id}, but no matching config found.")
                    
                    elif intent == 'approval':
                        if not user_email: continue
                        active_request = db_service.find_active_request_by_user(user_email)
                        if active_request:
                            target_config = next((c for c in associated_configs if c['config_id'] == active_request['config_id']), None)
                            if target_config:
                                full_workflow_config = {**static_config, **mailbox_config, **target_config}
                                handle_approval_email(
                                    from_email=sender_email,
                                    user_email=user_email,
                                    requested_group=active_request['requested_group'],
                                    config=full_workflow_config
                                )
                            else:
                                logging.warning(f"Approval for '{user_email}' found, but their active request (config_id: {active_request['config_id']}) is not associated with this mailbox (ID: {mailbox_id}).")
                        else:
                            logging.warning(f"Approval received for user '{user_email}' but no active request was found.")

                except Exception as e:
                    logging.error(f"[Mailbox ID: {mailbox_id}] Exception processing UID {uid_str}: {e}", exc_info=True)

            db_service.update_last_check_time(last_check_timestamp_key, datetime.now().isoformat())
    except Exception as e:
        logging.error(f"[Mailbox ID: {mailbox_id}] Could not access mailbox: {e}", exc_info=True)

# ==============================================================================
# Phase 2: Action Worker (Wait & Execute)
# ==============================================================================

def process_pending_actions(config):
    """
    PHASE 2: Queries for "matured" requests for a specific config and acts on them.
    """
    config_id = config['config_id']
    try:
        mature_requests = db_service.get_mature_unprocessed_requests(config_id, config)
        if mature_requests:
             logging.info(f"[{config_id}] Found {len(mature_requests)} mature requests to process.")
        
        for request in mature_requests:
            user_email = request['user_to_onboard_email']
            requested_group = request['requested_group']
            logging.info(f"[{config_id}] Evaluating mature request for user '{user_email}'.")

            # ===== FIX STARTS HERE =====
            # Bug Fix: The logic here now mirrors the robust, recursive logic of handle_approval_email
            # to ensure stages are advanced correctly even in proactive approval scenarios.
            
            # Check if any approvers can be found. If not, it's a config error.
            required_approvers = db_service.get_required_approvers_for_stage(request, config)
            if not required_approvers:
                error_msg = f"Could not find any required approvers for stage {request['current_stage']}. Check AD configuration."
                logging.error(f"[{config_id}] {error_msg}")
                db_service.update_request_status_composite(user_email, requested_group, config_id, 'error', error_msg)
                continue 

            # Check if there are still approvers missing for the current stage.
            missing_approvers = db_service.get_missing_approvers_for_stage(request, config)
            
            if not missing_approvers:
                # This stage is pre-approved. We can now use the main approval handler
                # to advance the stage and re-evaluate, ensuring the full workflow runs.
                logging.info(f"[{config_id}] Stage {request['current_stage']} for '{user_email}' is pre-approved. Advancing workflow.")
                # We pass a dummy email for the 'from_email' as it's not needed for this logic path.
                handle_approval_email("system.preapproved@local", user_email, requested_group, config)
            else:
                # Approvals are missing, so we start the standard process by sending an email.
                logging.info(f"[{config_id}] Approvals missing for '{user_email}'. Initiating approval process.")
                send_request_to_next_stage(request, config)
            # ===== FIX ENDS HERE =====

    except Exception as e:
        logging.error(f"[{config_id}] Error processing pending actions: {e}", exc_info=True)

# --- Approval and Reminder Logic ---

def handle_approval_email(from_email, user_email, requested_group, config):
    """
    Handles an incoming approval email, records it, and advances the workflow.
    """
    config_id = config.get('config_id', 'DEFAULT')
    request = db_service.get_active_request(user_email, requested_group, config_id)
    if not request:
        logging.warning(f"[{config_id}] Approval received but no active request found for {user_email}.")
        return

    # A dummy email is used for the proactive flow, so we only validate real ones.
    if from_email != "system.preapproved@local":
        effective_approvers = db_service.get_effective_approvers_for_stage(request, config)
        if not effective_approvers:
            logging.error(f"[{config_id}] Could not determine effective approvers for request ID {request['id']}. Check AD config.")
            return
        if from_email not in effective_approvers:
            logging.warning(f"[{config_id}] Unauthorized approval attempt by '{from_email}' for request ID {request['id']}.")
            return
        db_service.add_stage_approval(request, from_email, config)

    # ===== FIX STARTS HERE =====
    # Bug Fix: This check now allows the system-triggered pre-approval flow to proceed,
    # breaking the infinite loop.
    if request['status'] == 'new_unprocessed' and from_email != "system.preapproved@local":
        logging.info(f"[{config_id}] Recorded proactive approval for staged request for {user_email}. Action worker will process.")
        return
    # ===== FIX ENDS HERE =====
        
    request = db_service.get_active_request(user_email, requested_group, config_id)
    
    missing = db_service.get_missing_approvers_for_stage(request, config)
    if not missing:
        logging.info(f"[{config_id}] Stage {request['current_stage']} for {user_email} is now fully approved.")
        
        if request['current_stage'] >= 2: # Assuming 2 is the final approval stage
            try:
                db_service.onboard_user_to_target_db(user_email, config)
                db_service.update_request_status_composite(user_email, requested_group, config_id, 'completed', 'Access granted after final approval.')
                send_confirmation_email(user_email, config)
            except Exception:
                logging.error(f"[{config_id}] Halting onboarding for '{user_email}' after final approval due to database error.")
        else:
            logging.info(f"[{config_id}] Advancing request {request['id']} to the next stage.")
            advanced_request = db_service.advance_to_next_stage_composite(user_email, requested_group, config_id)
            if advanced_request:
                # ===== FIX STARTS HERE =====
                # Bug Fix: Determine the actual approver from the previous stage to correctly
                # check approvals for the newly advanced stage.
                actual_approver = from_email
                if from_email == "system.preapproved@local":
                    # In the pre-approved flow, the approver is already in the database for the stage we just completed.
                    approvals_for_previous_stage = request.get('stage_approvals', {}).get(str(request['current_stage']), [])
                    if approvals_for_previous_stage:
                        actual_approver = approvals_for_previous_stage[0] # Use the first approver from the completed stage
                
                logging.info(f"[{config_id}] Re-evaluating approvals for newly advanced stage {advanced_request['current_stage']} using approver '{actual_approver}'.")
                handle_approval_email(actual_approver, user_email, requested_group, config)
                # ===== FIX ENDS HERE =====
    else:
        logging.info(f"[{config_id}] Approval from {from_email} recorded. Still waiting on: {missing}")

def process_pending_reminders(config):
    """Sends reminders for requests that have been pending for too long."""
    try:
        requests = db_service.get_pending_requests_for_reminder(config)
        for req in requests:
            missing_approvers = db_service.get_missing_approvers_for_stage(req, config)
            if missing_approvers:
                subject = f"REMINDER: Approval Required for {req['user_to_onboard_email']}"
                body = (
                    f"Hello,\n\nThis is a reminder that your approval is still required for user "
                    f"'{req['user_to_onboard_email']}' to access '{req['requested_group']}'.\n\n"
                    f"Thank you,\n{config['team_alias']}"
                )
                send_email(missing_approvers, subject, body, config)
                db_service.update_request_status_composite(
                    req['user_to_onboard_email'], req['requested_group'], req['config_id'],
                    req['status'], f"Reminder sent to: {', '.join(missing_approvers)}"
                )
    except Exception as e:
        logging.error(f"[{config['config_id']}] Get pending reminders failed: {e}")

# --- Email Sending Helpers ---

def send_confirmation_email(user_email, config):
    subject = f"Welcome! Your Access to {config['team_alias']} is Granted"
    body = f"Hello,\n\nYour account ({user_email}) has been successfully onboarded.\n\nBest regards,\n{config['team_alias']}"
    send_email([user_email], subject, body, config)

def send_request_to_next_stage(request, config):
    """Sends an approval request email to the required approvers for the current stage."""
    stage_num = request['current_stage']
    user_email = request['user_to_onboard_email']
    group = request['requested_group']
    
    approvers = db_service.get_required_approvers_for_stage(request, config)
    stage_name = "Manager" if stage_num == 1 else f"Stage {stage_num} Owner"
    
    if not approvers:
        logging.error(f"Could not find approvers for {stage_name} for user {user_email}.")
        db_service.update_request_status_composite(
            user_email, group, config['config_id'], 'error', f"No approvers found for stage {stage_num}"
        )
        return

    subject = f"ACTION REQUIRED: Approve Onboarding for {user_email}"
    body = (
        f"Hello,\n\nThe user '{user_email}' has requested access to the '{group}' system, "
        f"which requires your approval.\n\nPlease reply to this email with 'Approved' or 'Rejected'.\n\n"
        f"Thank you,\n{config['team_alias']}"
    )
    send_email(approvers, subject, body, config)
    
    new_status = f"pending_{stage_name.lower().replace(' ','_')}_approval"
    db_service.update_request_status_composite(
        user_email, group, config['config_id'], new_status, 
        f"Approval request sent to {stage_name}(s): {', '.join(approvers)}"
    )

# FILE: app/services/db_service.py
# ==============================================================================
# Database interaction layer for user onboarding requests.
# Manages all tables, including the 'mailboxes' table for credential
# grouping and the simplified 'configuration' table for workflow rules.
# ==============================================================================

import psycopg2
import logging
import json
import os
from psycopg2.extras import DictCursor
from datetime import datetime, timedelta
import mysql.connector
import oracledb
import pyodbc
from app.services import ad_service

def get_db_connection():
    """Establishes and returns a connection to the main PostgreSQL database."""
    try:
        return psycopg2.connect(
            dbname=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASS"),
            host=os.getenv("DB_HOST"),
            port=os.getenv("DB_PORT")
        )
    except Exception as e:
        logging.error(f"Failed to connect to application DB: {e}")
        raise

def setup_database():
    """Initializes all necessary tables and triggers in the database."""
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            # Stores unique mailbox credentials
            cur.execute("""
                CREATE TABLE IF NOT EXISTS mailboxes (
                    id SERIAL PRIMARY KEY,
                    description TEXT,
                    imap_server TEXT NOT NULL,
                    imap_user TEXT NOT NULL,
                    imap_pass TEXT NOT NULL,
                    smtp_server TEXT NOT NULL,
                    smtp_port INT NOT NULL,
                    smtp_user TEXT NOT NULL,
                    smtp_pass TEXT NOT NULL
                );
            """)

            # Stores workflow-specific rules and links to a mailbox
            cur.execute("""
                CREATE TABLE IF NOT EXISTS configuration (
                    config_id TEXT PRIMARY KEY,
                    description TEXT,
                    is_active BOOLEAN DEFAULT TRUE,
                    team_alias TEXT NOT NULL,
                    workflow_type TEXT NOT NULL DEFAULT 'ad_validated',
                    required_ad_group TEXT,
                    mailbox_id INTEGER REFERENCES mailboxes(id),
                    target_db_type TEXT,
                    target_db_config JSONB,
                    target_table_name TEXT,
                    target_column_mappings JSONB
                );
            """)

            # Other application tables (unchanged)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS onboarding_tracker (
                    id SERIAL PRIMARY KEY, user_to_onboard_email VARCHAR(255) NOT NULL,
                    requested_group VARCHAR(100) NOT NULL, config_id TEXT NOT NULL,
                    status VARCHAR(50) NOT NULL DEFAULT 'new_unprocessed', current_stage INT DEFAULT 1,
                    stage_approvals JSONB DEFAULT '{}'::jsonb, delegated_approvers JSONB DEFAULT '[]'::jsonb,
                    duplicate_of INTEGER, request_count INT DEFAULT 1, last_activity_details TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_onboard_email_group_config ON onboarding_tracker(user_to_onboard_email, requested_group, config_id);")
            cur.execute("CREATE TABLE IF NOT EXISTS onboarding_log (email TEXT NOT NULL, config_id TEXT NOT NULL, access_flag BOOLEAN DEFAULT FALSE, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (email, config_id));")
            cur.execute("CREATE TABLE IF NOT EXISTS processed_uids (uid TEXT PRIMARY KEY, processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP);")
            cur.execute("CREATE TABLE IF NOT EXISTS app_state (key TEXT PRIMARY KEY, value TEXT NOT NULL);")
            
            # Timestamp trigger (unchanged)
            cur.execute("""
                CREATE OR REPLACE FUNCTION trigger_set_timestamp() RETURNS TRIGGER AS $$
                BEGIN NEW.updated_at = NOW(); RETURN NEW; END;
                $$ LANGUAGE plpgsql;
            """)
            cur.execute("DROP TRIGGER IF EXISTS set_timestamp ON onboarding_tracker;")
            cur.execute("CREATE TRIGGER set_timestamp BEFORE UPDATE ON onboarding_tracker FOR EACH ROW EXECUTE PROCEDURE trigger_set_timestamp();")
            conn.commit()
            logging.info("Database setup checked and completed.")

def get_last_check_time(config_id, config):
    """
    Gets the last check timestamp for a given config, applying a 30-second safety overlap.
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            key = f"last_check_timestamp_{config_id}"
            cur.execute("SELECT value FROM app_state WHERE key = %s;", (key,))
            row = cur.fetchone()
            if row and row[0]:
                last_time = datetime.fromisoformat(row[0])
                overlapped_time = last_time - timedelta(seconds=30)
                logging.info(f"[{config_id}] Last check: {last_time}. Using overlap time: {overlapped_time}")
                return overlapped_time.isoformat()

            fallback = (datetime.now() - timedelta(days=config.get('INITIAL_LOOKBACK_DAYS', 1))).isoformat()
            logging.info(f"[{config_id}] No last check timestamp found, using fallback: {fallback}")
            return fallback

def update_last_check_time(config_id, timestamp_iso):
    """Updates the last check timestamp for a given config."""
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            key = f"last_check_timestamp_{config_id}"
            cur.execute("INSERT INTO app_state (key, value) VALUES (%s, %s) ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value;", (key, timestamp_iso))
            conn.commit()

def create_onboarding_request_composite(user_email, group, config_id, status='new_unprocessed', current_stage=1, stage_approvals=None):
    """
    Creates a new request record. Defaults to 'new_unprocessed' status for the new workflow.
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                "INSERT INTO onboarding_tracker (user_to_onboard_email, requested_group, config_id, status, current_stage, stage_approvals) VALUES (%s, %s, %s, %s, %s, %s) RETURNING id;",
                (user_email, group, config_id, status, current_stage, json.dumps(stage_approvals or {}))
            )
            request_id = cur.fetchone()[0]
            conn.commit()
            logging.info(f"Created request ID {request_id} for {user_email} with status '{status}'.")
            return request_id

def find_active_request_by_user(user_email):
    """Finds the most recent, non-completed/duplicate request for a user across all configs."""
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute(
                "SELECT * FROM onboarding_tracker WHERE user_to_onboard_email = %s AND status NOT IN ('completed', 'duplicate', 'error') ORDER BY updated_at DESC LIMIT 1;",
                (user_email,)
            )
            return cur.fetchone()

def get_mature_unprocessed_requests(config_id, config):
    """
    Fetches requests that are in 'new_unprocessed' state and older than the configured maturity delay.
    """
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            delay_minutes = config.get('MATURITY_DELAY_MINUTES', 5)
            cur.execute(
                "SELECT * FROM onboarding_tracker WHERE config_id = %s AND status = 'new_unprocessed' AND created_at < NOW() - INTERVAL '%s minutes';",
                (config_id, delay_minutes)
            )
            return cur.fetchall()

def onboard_user_to_target_db(user_email, config):
    """
    Handles the final step of provisioning the user in the target database.
    """
    update_internal_user_access(user_email, config['config_id'])
    if not all([config.get('target_db_type'), config.get('target_table_name'), config.get('target_column_mappings')]):
        logging.warning(f"[{config['config_id']}] No target DB configured, skipping final onboarding.")
        return
    conn = None
    try:
        db_type = config['target_db_type']
        db_config = config['target_db_config']
        if db_type == 'postgresql':
            conn = psycopg2.connect(**db_config)
        elif db_type == 'mysql':
            conn = mysql.connector.connect(**db_config)
        elif db_type == 'oracle':
            conn = oracledb.connect(**db_config)
        elif db_type == 'mssql':
            conn_str = ';'.join([f'{k}={v}' for k, v in db_config.items()])
            conn = pyodbc.connect(conn_str)
        else:
            logging.error(f"Unsupported target DB type: {db_type}")
            return

        cursor = conn.cursor()
        table = config['target_table_name']
        mappings = config['target_column_mappings']
        email_col = mappings['email_column']

        cursor.execute(f"SELECT {email_col} FROM {table} WHERE {email_col} = %s", (user_email,))
        exists = cursor.fetchone()

        default_values = {mappings[k.replace('default_', '') + '_column']: v for k, v in mappings.items() if k.startswith('default_')}

        if exists:
            update_cols = {**default_values, **{mappings['active_column']: True}}
            set_clause = ', '.join([f"{col} = %s" for col in update_cols.keys()])
            query = f"UPDATE {table} SET {set_clause} WHERE {email_col} = %s"
            params = list(update_cols.values()) + [user_email]
        else:
            insert_cols = {**default_values, **{mappings['email_column']: user_email, mappings['active_column']: True}}
            cols_clause = ', '.join(insert_cols.keys())
            placeholders = ', '.join(['%s'] * len(insert_cols))
            query = f"INSERT INTO {table} ({cols_clause}) VALUES ({placeholders})"
            params = list(insert_cols.values())

        cursor.execute(query, params)
        conn.commit()
        logging.info(f"User {user_email} onboarded in target table {table}")
    except Exception as e:
        logging.error(f"Onboarding to target DB failed: {e}", exc_info=True)
        if conn:
            conn.rollback()
        # ===== FIX =====
        # Re-raise the exception so the calling function knows the operation failed.
        raise
    finally:
        if conn:
            conn.close()

def get_required_approvers_for_stage(request, config):
    """Gets the base list of required approvers from AD (manager or group owners)."""
    stage = request['current_stage']
    if stage == 1:
        mgr = ad_service.get_user_manager(request['user_to_onboard_email'], config)
        return [mgr.lower()] if mgr else []
    elif stage == 2:
        return ad_service.get_group_owners(config['required_ad_group'], config)
    return []

def get_effective_approvers_for_stage(request, config):
    """
    Calculates the list of people who can actually approve, including delegations.
    """
    required = set(get_required_approvers_for_stage(request, config))
    delegations_raw = request.get('delegated_approvers', '[]')
    delegations = json.loads(delegations_raw) if isinstance(delegations_raw, str) else delegations_raw
    
    if not delegations:
        return list(required)
        
    mapping = {item['original'].lower(): item['delegate'].lower() for item in delegations}
    effective = set()
    for approver in required:
        effective.add(mapping.get(approver, approver))
    return list(effective)

def get_missing_approvers_for_stage(request, config):
    """Compares effective approvers with those who have already approved."""
    effective = set(get_effective_approvers_for_stage(request, config))
    stage_approvals = request.get('stage_approvals', {})
    approved_for_stage = set(stage_approvals.get(str(request['current_stage']), []))
    return list(effective - approved_for_stage)

def add_stage_approval(request, approver_email, config):
    """Adds an approver's email to the JSONB field for the current stage of a request."""
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            stage_str = str(request['current_stage'])
            stage_approvals = request.get('stage_approvals', {})
            current_approvals = stage_approvals.get(stage_str, [])
            
            if approver_email in current_approvals:
                return False

            stage_approvals[stage_str] = current_approvals + [approver_email.lower()]
            cur.execute(
                "UPDATE onboarding_tracker SET stage_approvals = %s::jsonb, last_activity_details = %s WHERE id = %s;",
                (json.dumps(stage_approvals), f"Approval recorded from {approver_email}", request['id'])
            )
            conn.commit()
            logging.info(f"Added approval from {approver_email} for request ID {request['id']} at stage {stage_str}.")
            return cur.rowcount > 0

def update_request_status_composite(user_email, group, config_id, status, details):
    """Updates the status and details of the active request for a user/group/config."""
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                "UPDATE onboarding_tracker SET status=%s, last_activity_details=%s WHERE user_to_onboard_email=%s AND requested_group=%s AND config_id=%s AND status != 'duplicate';",
                (status, details, user_email, group, config_id)
            )
            conn.commit()

def get_active_request(user_email, group, config_id):
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("SELECT * FROM onboarding_tracker WHERE user_to_onboard_email=%s AND requested_group=%s AND config_id=%s AND status NOT IN ('completed', 'duplicate', 'error') ORDER BY created_at DESC LIMIT 1;", (user_email, group, config_id))
            return cur.fetchone()

def advance_to_next_stage_composite(user_email, group, config_id):
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("UPDATE onboarding_tracker SET current_stage = current_stage + 1 WHERE user_to_onboard_email = %s AND requested_group = %s AND config_id = %s AND status NOT IN ('duplicate', 'completed') RETURNING *;", (user_email, group, config_id))
            req = cur.fetchone()
            conn.commit()
            return req

def get_pending_requests_for_reminder(config):
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            reminder_hours = config.get('REMINDER_THRESHOLD_HOURS', 24)
            cur.execute("SELECT * FROM onboarding_tracker WHERE status LIKE 'pending_%%' AND updated_at < NOW() - INTERVAL '%s hours' AND config_id = %s;", (reminder_hours, config['config_id']))
            return cur.fetchall()

def claim_uid_for_processing(uid):
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute("INSERT INTO processed_uids (uid) VALUES (%s) ON CONFLICT (uid) DO NOTHING;", (uid,))
            conn.commit()
            return cur.rowcount > 0

def get_all_active_configurations():
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("SELECT * FROM configuration WHERE is_active = TRUE;")
            return cur.fetchall()

def update_internal_user_access(user_email, config_id):
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute("INSERT INTO onboarding_log (email, config_id, access_flag) VALUES (%s, %s, TRUE) ON CONFLICT (email, config_id) DO UPDATE SET access_flag=TRUE;", (user_email, config_id))
            conn.commit()

def get_mailbox_config_by_id(mailbox_id):
    """Fetches a single mailbox configuration by its ID."""
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            cur.execute("SELECT * FROM mailboxes WHERE id = %s;", (mailbox_id,))
            return cur.fetchone()

# FILE: app/services/ai_service.py
# ==============================================================================
# AI service for analyzing email content using Ollama.
# Determines intent (new_request, approval, etc.) and extracts key information.
# ==============================================================================
import logging
import json
import re
import ollama

client = None

def is_real_user_email(email_address):
    """Helper to filter out no-reply/bot/system email addresses."""
    if not email_address: return False
    patterns = [
        r'no-?reply@', r'notification', r'do-?not-?reply@', r'mailer-daemon', r'postmaster@',
        r'automated', r'helpdesk', r'bounces@', r'^noreply', r'bot@', r'listserv',
        r'system@', r'alerts?@'
    ]
    return not any(re.search(pat, email_address, re.IGNORECASE) for pat in patterns)

KEYWORDS = ['onboard', 'request access', 'join', 'add access', 'add to group', 'registration', 'enable access', 'new user', 'account setup', 'provision', 'grant access', 'request membership', 'add user']

def contains_onboarding_keyword(text):
    """Checks if text contains common onboarding-related keywords."""
    return any(kw in text.lower() for kw in KEYWORDS)

def analyze_email(subject, body, config):
    """
    Analyzes email content using an LLM to determine intent and extract entities.
    """
    global client
    if client is None:
        logging.info(f"Initializing Ollama client with host: {config['OLLAMA_HOST']}")
        client = ollama.Client(host=config['OLLAMA_HOST'])

    full_content = f"Subject: {subject}\n\nBody:\n{body}"
    compacted = re.sub(r'\s+', ' ', full_content).strip()[:5000]
    
    # RESTORED: Using the original, more detailed prompt for higher accuracy.
    system_prompt = """
You are a careful IT onboarding gatekeeper. Your job is to classify incoming emails.
You MUST ONLY return a JSON dictionary, and nothing else.

A "new_request" is valid ONLY when BOTH:
* The email expresses a clear onboarding intent (contains keywords like "onboard", "request access", "add user", "join", "add to group", "enable access", "registration"), AND
* Contains a real person's email address (NOT no-reply, notification, bot, mailer-daemon, etc).

If BOTH these conditions are not met, classify as intent "query" and set all extracted fields to null.

JSON format for output:

For onboarding requests:
  {
      "intent": "new_request",
      "user_email": "[REAL_EMAIL]",
      "requested_group": "[GROUP]" // The Team/System requested, e.g. "DEV". If you can't find it, set it to null.
  }

For everything else:
  {
    "intent": "query",
    "user_email": null,
    "delegate_email": null,
    "requested_group": null
  }
Do NOT guess or invent values. ONLY extract real emails from the body or subject and carefully check the sender.
If the only email present is a no-reply, notification, daemon, or other bot/system address, DO NOT create new_request.
Never use example.com or placeholder values.

For approvals and other flows: (same as before)
      {"intent": "[approval_or_rejection]", "user_email": "[USER]", "requested_group": "[GROUP_NAME]"}
      (if not found, use nulls)

If you see an out of office response that names a delegate, return:
      {"intent": "out_of_office", "delegate_email": "[DELEGATE_EMAIL]"}

REMEMBER:
* If the message does not contain onboarding keywords AND a real user email, classify as "query".
"""

    try:
        response = client.chat(
            model=config['OLLAMA_MODEL'],
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': compacted}
            ],
            options={'temperature': 0.0},
            format='json'
        )
        result_json_str = response['message']['content']
        res = json.loads(result_json_str)

        # Post-processing validation to ensure high-quality results
        user_email = res.get('user_email')
        if res.get('intent') == 'new_request':
            if not (is_real_user_email(user_email) and contains_onboarding_keyword(subject + ' ' + body)):
                logging.warning(f"AI classified as 'new_request' but failed validation. Reverting to 'query'.")
                res['intent'] = "query"
                res['user_email'] = None
                res['requested_group'] = None
                
        logging.info(f"AI Analysis Result: {json.dumps(res)}")
        return res

    except Exception as e:
        logging.error(f"Error calling Ollama or parsing its response: {e}", exc_info=True)
        return None

-- ==============================================================================
-- SQL Schema for the Onboarding Automation Application
-- ==============================================================================

-- Drop tables in reverse order of creation to handle dependencies if they already exist.
DROP TRIGGER IF EXISTS set_timestamp ON public.onboarding_tracker;
DROP FUNCTION IF EXISTS public.trigger_set_timestamp();
DROP TABLE IF EXISTS public.app_state;
DROP TABLE IF EXISTS public.processed_uids;
DROP TABLE IF EXISTS public.onboarding_log;
DROP TABLE IF EXISTS public.onboarding_tracker;
DROP TABLE IF EXISTS public.configuration;
DROP TABLE IF EXISTS public.mailboxes;


-- ==============================================================================
-- Table: mailboxes
-- Purpose: Stores unique mailbox credentials. This allows multiple workflows
-- to share the same mailbox or use their own independent ones.
-- ==============================================================================
CREATE TABLE public.mailboxes (
    id SERIAL PRIMARY KEY,
    description TEXT,
    imap_server TEXT NOT NULL,
    imap_user TEXT NOT NULL,
    imap_pass TEXT NOT NULL,
    smtp_server TEXT NOT NULL,
    smtp_port INTEGER NOT NULL,
    smtp_user TEXT NOT NULL,
    smtp_pass TEXT NOT NULL
);

COMMENT ON TABLE public.mailboxes IS 'Stores unique credentials for each email account the system needs to access.';
COMMENT ON COLUMN public.mailboxes.id IS 'Unique identifier for the mailbox configuration.';


-- ==============================================================================
-- Table: configuration
-- Purpose: Stores the specific rules for each onboarding workflow.
-- ==============================================================================
CREATE TABLE public.configuration (
    config_id TEXT PRIMARY KEY,
    description TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    team_alias TEXT NOT NULL,
    workflow_type TEXT NOT NULL DEFAULT 'ad_validated',
    required_ad_group TEXT,
    mailbox_id INTEGER REFERENCES public.mailboxes(id) ON DELETE SET NULL,
    target_db_type TEXT,
    target_db_config JSONB,
    target_table_name TEXT,
    target_column_mappings JSONB
);

COMMENT ON TABLE public.configuration IS 'Defines the rules for each distinct onboarding workflow (e.g., for DEV team, DBA team).';
COMMENT ON COLUMN public.configuration.config_id IS 'Unique identifier for the workflow configuration (e.g., DBA_Onboarding).';
COMMENT ON COLUMN public.configuration.mailbox_id IS 'Foreign key linking to the specific mailbox credentials in the mailboxes table.';


-- ==============================================================================
-- Table: onboarding_tracker
-- Purpose: Tracks the state of every individual onboarding request.
-- ==============================================================================
CREATE TABLE public.onboarding_tracker (
    id SERIAL PRIMARY KEY,
    user_to_onboard_email VARCHAR(255) NOT NULL,
    requested_group VARCHAR(100) NOT NULL,
    config_id TEXT NOT NULL,
    status VARCHAR(50) NOT NULL DEFAULT 'new_unprocessed',
    current_stage INTEGER DEFAULT 1,
    stage_approvals JSONB DEFAULT '{}'::jsonb,
    delegated_approvers JSONB DEFAULT '[]'::jsonb,
    duplicate_of INTEGER,
    request_count INTEGER DEFAULT 1,
    last_activity_details TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Composite index to speed up lookups for active requests.
CREATE INDEX IF NOT EXISTS idx_onboard_email_group_config
ON public.onboarding_tracker(user_to_onboard_email, requested_group, config_id);

COMMENT ON TABLE public.onboarding_tracker IS 'Tracks the status and approval history of each individual onboarding request.';
COMMENT ON COLUMN public.onboarding_tracker.status IS 'The current state of the request (e.g., new_unprocessed, pending_manager_approval, completed).';


-- ==============================================================================
-- Table: onboarding_log
-- Purpose: A permanent record of users who have been granted access.
-- ==============================================================================
CREATE TABLE public.onboarding_log (
    email TEXT NOT NULL,
    config_id TEXT NOT NULL,
    access_flag BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    -- Composite primary key ensures a user can only have one entry per configuration.
    PRIMARY KEY (email, config_id)
);

COMMENT ON TABLE public.onboarding_log IS 'Maintains a final record of which users have been successfully onboarded to which system.';


-- ==============================================================================
-- Table: processed_uids
-- Purpose: Prevents the system from processing the same email more than once.
-- ==============================================================================
CREATE TABLE public.processed_uids (
    uid TEXT PRIMARY KEY,
    processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE public.processed_uids IS 'Stores the unique IDs (UIDs) of emails that have already been processed to prevent duplicates.';


-- ==============================================================================
-- Table: app_state
-- Purpose: Stores persistent application state, like the last email check time.
-- ==============================================================================
CREATE TABLE public.app_state (
    key TEXT PRIMARY KEY,
    value TEXT NOT NULL
);

COMMENT ON TABLE public.app_state IS 'A simple key-value store for application state, such as last_check_timestamps for each mailbox.';


-- ==============================================================================
-- Function and Trigger: trigger_set_timestamp
-- Purpose: Automatically updates the 'updated_at' column in the
-- onboarding_tracker table whenever a row is modified.
-- ==============================================================================
CREATE OR REPLACE FUNCTION public.trigger_set_timestamp()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = NOW();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER set_timestamp
BEFORE UPDATE ON public.onboarding_tracker
FOR EACH ROW
EXECUTE FUNCTION public.trigger_set_timestamp();

COMMENT ON TRIGGER set_timestamp ON public.onboarding_tracker IS 'Automatically updates the updated_at field on row modification.';



-- Step 1: Create the shared mailbox record in the new 'mailboxes' table.
-- This will create a record with id = 1, which we will reference below.
INSERT INTO public.mailboxes
(description, imap_server, imap_user, imap_pass, smtp_server, smtp_port, smtp_user, smtp_pass)
VALUES
('Tech Teams Shared Mailbox', 'imap.gmail.com', 'facilebase@gmail.com', 'fjxp etsp bhzw bbnz', 'smtp.gmail.com', 587, 'facilebase@gmail.com', 'fjxp etsp bhzw bbnz');


-- Step 2: Create the workflow configurations that USE the shared mailbox.
-- Notice both configurations now point to 'mailbox_id = 1'.

-- Configuration for the DBA Team
INSERT INTO public.configuration
(config_id, description, is_active, team_alias, workflow_type, required_ad_group, mailbox_id, target_db_type, target_db_config, target_table_name, target_column_mappings)
VALUES
('DBA_Onboarding', 'Onboarding for the DBA Team', true, 'DBA Team', 'ad_validated', 'DB', 1, 'postgresql', '{"host": "localhost", "port": 5432, "user": "postgres", "dbname": "onboarding_db", "password": "RSK12@postgres"}', 'db_users', '{"email_column": "email", "active_column": "active", "default_access_level": "ro"}');

-- Configuration for the DEV Team
INSERT INTO public.configuration
(config_id, description, is_active, team_alias, workflow_type, required_ad_group, mailbox_id, target_db_type, target_db_config, target_table_name, target_column_mappings)
VALUES
('DEV_Onboarding', 'Onboarding for the DEV Team', true, 'DEV Team', 'ad_validated', 'DEV', 1, 'postgresql', '{"host": "localhost", "port": 5432, "user": "postgres", "dbname": "onboarding_db", "password": "RSK12@postgres"}', 'dev_users', '{"email_column": "email", "active_column": "active", "default_access_level": "rw"}');

UPDATE public.configuration
SET 
    target_column_mappings = '{"email_column": "email", "active_column": "active", "access_level_column": "access_level", "default_access_level": "rw"}'
WHERE 
    config_id = 'DEV_Onboarding';

UPDATE public.configuration
SET 
    target_column_mappings = '{"email_column": "email", "active_column": "active", "access_level_column": "access_level", "default_access_level": "ro"}'
WHERE 
    config_id = 'DBA_Onboarding';


-- This script adds the missing 'access_level' column to your target user tables.

-- Add the column to the dev_users table
ALTER TABLE public.dev_users
ADD COLUMN access_level VARCHAR(50);

-- Add the column to the db_users table (for your DBA_Onboarding config)
ALTER TABLE public.db_users
ADD COLUMN access_level VARCHAR(50);

